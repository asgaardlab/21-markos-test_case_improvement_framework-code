{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and pre-processing data\n",
    "Notebook with code to read and apply pre-processing steps to test case descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requirements\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import statistics as st\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "from statistics import mean\n",
    "import string\n",
    "\n",
    "import collections\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from expects import (contain_exactly, equal, expect, have_keys)\n",
    "import attr\n",
    "from functools import partial\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManipulator:\n",
    "    \"\"\"Class with methods to read and pre-process data\n",
    "    \"\"\"            \n",
    "    def __init__(self, my_data_dir: str) -> None:\n",
    "        self.data_dir = my_data_dir\n",
    "        \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        data_files = [os.path.join(root, name)\n",
    "                     for root, dirs, files in os.walk(self.data_dir)\n",
    "                     for name in files\n",
    "                     if name.endswith((\".xlsx\"))]\n",
    "        \n",
    "        # Declare pandas df to be populated\n",
    "        column_names = [\"Type\", \"Key\", \"Name\", \"Objective\", \"Labels\", \"Step_ID\", \"Step\"]\n",
    "        test_steps_df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "        # Index to add data to the df\n",
    "        index_to_add = 0\n",
    "\n",
    "        print(\"Reading input data...\")   \n",
    "        for test_file in data_files:\n",
    "            # load data and iterate through it to select only the columns we are interested in\n",
    "            test_data_df = pd.read_excel(test_file)\n",
    "            for index, row in test_data_df.iterrows():\n",
    "                current_type = row[\"Type\"]\n",
    "                current_key = row[\"Key\"]\n",
    "                current_name = row[\"Name\"]\n",
    "                current_objective = row[\"Objective\"]\n",
    "                current_labels = row[\"Labels\"]\n",
    "                current_step_id = row[\"Step_ID\"]\n",
    "                current_steps = row[\"Step\"]\n",
    "                test_steps_df.loc[index_to_add] = [current_type, current_key, current_name, current_objective, current_labels, current_step_id, current_steps]\n",
    "                index_to_add += 1\n",
    "        print(\"Done!\")\n",
    "        print(\"Shape of data: \", test_steps_df.shape)\n",
    "        return test_steps_df\n",
    "        \n",
    "    # Function to pre-process field (test case name, objective, or steps), such as tokenization, stop word removal, lemmatization\n",
    "    def preprocess_test_data(self, df, field) -> pd.DataFrame:\n",
    "        print(\"Pre-processing test cases...\")\n",
    "        \n",
    "        # Lower case \n",
    "        df[field] = df[field].apply(lambda x: x.lower() if not (pd.isnull(x)) else x)\n",
    "        \n",
    "        # Replace urls with the keyword 'URL'\n",
    "        df[field] = df[field].apply(lambda x: re.sub(r'http\\S+', ' URL ', x) if not (pd.isnull(x)) else x)\n",
    "\n",
    "        # remove underscores\n",
    "        df[field] = df[field].apply(lambda x: re.sub('_', ' ', x).strip() if not (pd.isnull(x)) else x)\n",
    "        \n",
    "        # Remove digits and words with digits\n",
    "        df[field] = df[field].apply(lambda x: re.sub('\\w*\\d\\w*','', x) if not (pd.isnull(x)) else x)\n",
    "        \n",
    "        # Remove dashes\n",
    "        df[field] = df[field].apply(lambda x: re.sub(' - ',' ', x) if not (pd.isnull(x)) else x)\n",
    "        df[field] = df[field].apply(lambda x: re.sub('- ',' ', x) if not (pd.isnull(x)) else x)\n",
    "        df[field] = df[field].apply(lambda x: re.sub(' -',' ', x) if not (pd.isnull(x)) else x)\n",
    "        \n",
    "        # Remove the remaining punctuations\n",
    "        df[field] = df[field].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation.replace('-','')), ' ', x) if not (pd.isnull(x)) else x)\n",
    "\n",
    "        # Remove extra spaces\n",
    "        df[field] = df[field].apply(lambda x: re.sub(' +',' ',x) if not (pd.isnull(x)) else x)\n",
    "\n",
    "        # Tokenization\n",
    "        toknz = nltk.tokenize.TweetTokenizer() # use tweet tokenizer as it does not split apostrophes\n",
    "        df[field] = df[field].apply(lambda x: toknz.tokenize(x) if not (pd.isnull(x)) else x)\n",
    "           \n",
    "        # Stopword removal\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        df[field] = df[field].apply(lambda x: [w for w in x if not w in stop_words] if not (np.all(pd.isnull(x))) else x)\n",
    "        \n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        df[field] = df[field].apply(lambda x: [lemmatizer.lemmatize(w) for w in x] if not (np.all(pd.isnull(x))) else x)\n",
    "\n",
    "        print(\"Pre-processing finished!\")\n",
    "        return df\n",
    "    \n",
    "    # Function to pre-process field (test case name, objective, or steps) without 'stop word removal' and 'lemmatization' (to be used in the language models)\n",
    "    def preprocess_lightweight_test_data(self, df, field) -> pd.DataFrame:\n",
    "        print(\"Pre-processing test cases...\")\n",
    "        \n",
    "        # Lower case \n",
    "        df[field] = df[field].apply(lambda x: x.lower() if not (pd.isnull(x)) else x)\n",
    "\n",
    "        # Replace urls with the keyword 'URL'\n",
    "        df[field] = df[field].apply(lambda x: re.sub(r'http\\S+', ' URL ', x) if not (pd.isnull(x)) else x)\n",
    "\n",
    "        # remove underscores\n",
    "        df[field] = df[field].apply(lambda x: re.sub('_', ' ', x).strip() if not (pd.isnull(x)) else x)\n",
    "        \n",
    "        # Remove digits and words with digits\n",
    "        df[field] = df[field].apply(lambda x: re.sub('\\w*\\d\\w*','', x) if not (pd.isnull(x)) else x)\n",
    "        \n",
    "        # Remove dashes\n",
    "        df[field] = df[field].apply(lambda x: re.sub(' - ',' ', x) if not (pd.isnull(x)) else x)\n",
    "        df[field] = df[field].apply(lambda x: re.sub('- ',' ', x) if not (pd.isnull(x)) else x)\n",
    "        df[field] = df[field].apply(lambda x: re.sub(' -',' ', x) if not (pd.isnull(x)) else x)\n",
    "        \n",
    "        # Remove the remaining punctuations\n",
    "        df[field] = df[field].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation.replace('-','')), ' ', x) if not (pd.isnull(x)) else x)\n",
    "\n",
    "        # Remove extra spaces\n",
    "        df[field] = df[field].apply(lambda x: re.sub(' +',' ',x) if not (pd.isnull(x)) else x)\n",
    "\n",
    "        # Tokenization\n",
    "        toknz = nltk.tokenize.TweetTokenizer() # use tweet tokenizer as it does not split apostrophes\n",
    "        df[field] = df[field].apply(lambda x: toknz.tokenize(x) if not (pd.isnull(x)) else x)\n",
    "\n",
    "        print(\"Lighweight pre-processing finished!\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "data_dir = '/dataset/unpreprocessed_data'\n",
    "\n",
    "# Instantiate data manipulator class\n",
    "data_manipulator = DataManipulator(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and visualize head\n",
    "test_step_df = data_manipulator.load_data()\n",
    "test_step_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the data so that we can apply the lighweight version of the pre-processing steps\n",
    "test_step_stopwords_df = test_step_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the data (full pre-processing)\n",
    "test_step_df = data_manipulator.preprocess_test_data(test_step_df, \"Name\")\n",
    "test_step_df = data_manipulator.preprocess_test_data(test_step_df, \"Objective\")\n",
    "test_step_df = data_manipulator.preprocess_test_data(test_step_df, \"Step\")\n",
    "test_step_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lighweight pre-process of the data (keep stopwords and does not perform lemmatization)\n",
    "test_step_stopwords_df = data_manipulator.preprocess_lightweight_test_data(test_step_stopwords_df, \"Name\")\n",
    "test_step_stopwords_df = data_manipulator.preprocess_lightweight_test_data(test_step_stopwords_df, \"Objective\")\n",
    "test_step_stopwords_df = data_manipulator.preprocess_lightweight_test_data(test_step_stopwords_df, \"Step\")\n",
    "test_step_stopwords_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate test steps, names, and objectives to build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate steps, name, and objective\n",
    "test_steps_data = []\n",
    "test_steps_data.extend(test_step_df['Step'].tolist())\n",
    "test_steps_data.extend(test_step_df['Name'].tolist())\n",
    "test_steps_data.extend(test_step_df['Objective'].tolist())\n",
    "print(len(test_steps_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add beginning and end of sentence indicators\n",
    "for index, step in enumerate(test_steps_data):\n",
    "    test_steps_data[index] = ['[START]'] + step + ['[END]']\n",
    "    \n",
    "# Save tokenized steps\n",
    "tokenized_steps = test_steps_data\n",
    "with open(\"training_testing_data/with_name_objective/tokenized_steps.txt\", 'w') as write_handle:\n",
    "    for step in tokenized_steps:\n",
    "        write_handle.write(','.join(step))\n",
    "        write_handle.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate steps, name, and objective for data with stop words\n",
    "test_steps_data_stopwords = []\n",
    "test_steps_data_stopwords.extend(test_step_stopwords_df['Step'].tolist())\n",
    "test_steps_data_stopwords.extend(test_step_stopwords_df['Name'].tolist())\n",
    "test_steps_data_stopwords.extend(test_step_stopwords_df['Objective'].tolist())\n",
    "print(len(test_steps_data_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add beginning and end of sentence indicators\n",
    "for index, step in enumerate(test_steps_data_stopwords):\n",
    "    test_steps_data_stopwords[index] = ['[START]'] + step + ['[END]']\n",
    "\n",
    "# Save tokenized stopwords steps\n",
    "tokenized_steps_stopwords = test_steps_data_stopwords\n",
    "with open(\"training_testing_data/with_name_objective/tokenized_steps_stopwords.txt\", 'w') as write_handle:\n",
    "    for step in tokenized_steps_stopwords:\n",
    "        write_handle.write(','.join(step))\n",
    "        write_handle.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "print(len(tokenized_steps))\n",
    "tokenized_steps.sort()\n",
    "tokenized_steps = list(tokenized_steps for tokenized_steps,_ in itertools.groupby(tokenized_steps))\n",
    "print(len(tokenized_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save unique tokenized steps\n",
    "with open(\"training_testing_data/with_name_objective/tokenized_steps_unique.txt\", 'w') as write_handle:\n",
    "    for step in tokenized_steps:\n",
    "        write_handle.write(','.join(step))\n",
    "        write_handle.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates for data with stop words\n",
    "print(len(tokenized_steps_stopwords))\n",
    "tokenized_steps_stopwords.sort()\n",
    "tokenized_steps_stopwords = list(tokenized_steps_stopwords for tokenized_steps_stopwords,_ in itertools.groupby(tokenized_steps_stopwords))\n",
    "print(len(tokenized_steps_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save unique tokenized steps with stopwords\n",
    "with open(\"training_testing_data/with_name_objective/tokenized_steps_stopwords_unique.txt\", 'w') as write_handle:\n",
    "    for step in tokenized_steps_stopwords:\n",
    "        write_handle.write(','.join(step))\n",
    "        write_handle.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing and save as text file\n",
    "random.seed(12)\n",
    "shuffled_data = random.sample(tokenized_steps, k=len(tokenized_steps))\n",
    "training_size = int(len(shuffled_data) * 0.8)\n",
    "\n",
    "# Get training and testing data\n",
    "training_data = shuffled_data[0:training_size]\n",
    "testing_data = shuffled_data[training_size:]\n",
    "\n",
    "# Get validation data from training data (only for BERT fine-tuning)\n",
    "training_bert_size = int(len(training_data) * 0.8)\n",
    "training_data_bert = training_data[0:training_bert_size]\n",
    "validation_data_bert = training_data[training_bert_size:]\n",
    "\n",
    "# Remove [START] and [END] as BERT adds its own sentence beginning and ending tokens\n",
    "training_data_bert = [x[1:-1] for x in training_data_bert]\n",
    "validation_data_bert = [x[1:-1] for x in validation_data_bert]\n",
    "\n",
    "# Save all the obtained sets\n",
    "with open('training_testing_data/with_name_objective/training_data.txt', 'w') as write_handle:\n",
    "    for step in training_data:\n",
    "        write_handle.write(','.join(step))\n",
    "        write_handle.write('\\n')\n",
    "        \n",
    "with open('training_testing_data/with_name_objective/testing_data.txt', 'w') as write_handle:\n",
    "    for step in testing_data:\n",
    "        write_handle.write(','.join(step))\n",
    "        write_handle.write('\\n')\n",
    "        \n",
    "with open('training_testing_data/with_name_objective/training_data_bert.txt', 'w') as write_handle:\n",
    "    for step in training_data_bert:\n",
    "        write_handle.write(','.join(step))\n",
    "        write_handle.write('\\n')\n",
    "        \n",
    "with open('training_testing_data/with_name_objective/validation_data_bert.txt', 'w') as write_handle:\n",
    "    for step in validation_data_bert:\n",
    "        write_handle.write(','.join(step))\n",
    "        write_handle.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing for data with stop words and save as text file\n",
    "random.seed(12)\n",
    "shuffled_data = random.sample(tokenized_steps_stopwords, k=len(tokenized_steps_stopwords))\n",
    "training_size = int(len(shuffled_data) * 0.8)\n",
    "\n",
    "# Get training and testing data\n",
    "training_data_stopwords = shuffled_data[0:training_size]\n",
    "testing_data_stopwords = shuffled_data[training_size:]\n",
    "\n",
    "# Get validation data from training data (only for BERT fine-tuning)\n",
    "training_bert_size = int(len(training_data_stopwords) * 0.8)\n",
    "\n",
    "training_data_stopwords_bert = training_data_stopwords[0:training_bert_size]\n",
    "validation_data_stopwords_bert = training_data_stopwords[training_bert_size:]\n",
    "\n",
    "# Remove [START] and [END] as BERT adds its own sentence beginning and ending tokens\n",
    "training_data_stopwords_bert = [x[1:-1] for x in training_data_stopwords_bert]\n",
    "validation_data_stopwords_bert = [x[1:-1] for x in validation_data_stopwords_bert]\n",
    "\n",
    "with open('training_testing_data/with_name_objective/training_data_stopwords.txt', 'w') as write_handle:\n",
    "    for step in training_data_stopwords:\n",
    "        write_handle.write(','.join(step))\n",
    "        write_handle.write('\\n')\n",
    "        \n",
    "with open('training_testing_data/with_name_objective/testing_data_stopwords.txt', 'w') as write_handle:\n",
    "    for step in testing_data_stopwords:\n",
    "        write_handle.write(','.join(step))\n",
    "        write_handle.write('\\n')\n",
    "        \n",
    "with open('training_testing_data/with_name_objective/training_data_stopwords_bert.txt', 'w') as write_handle:\n",
    "    for step in training_data_stopwords_bert:\n",
    "        write_handle.write(','.join(step))\n",
    "        write_handle.write('\\n')\n",
    "        \n",
    "with open('training_testing_data/with_name_objective/validation_data_stopwords_bert.txt', 'w') as write_handle:\n",
    "    for step in validation_data_stopwords_bert:\n",
    "        write_handle.write(','.join(step))\n",
    "        write_handle.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save pandas df with existing test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_test_cases_df.to_pickle('training_testing_data/existing_test_cases.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
