{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology improvement analysis through language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we perform experiments with different language models to improve the terminology of manual test case descriptions. We use the following types of language models:\n",
    "\n",
    "* Statistical language models:\n",
    "  * Unidirectional unigram, bigram, trigram, 4-gram, 5-gram\n",
    "  * Bidirectional unigram, bigram, trigram, 4-gram, 5-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import statistics as st\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "from statistics import mean\n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "import collections\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from expects import (contain_exactly, equal, expect, have_keys)\n",
    "import attr\n",
    "from functools import partial\n",
    "from tabulate import tabulate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.offline as offline\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-processed data\n",
    "For the terminology improvement module, we do not perform stop word removal and lemmatization for the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'training_testing_data/with_name_objective/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "read_handle = open(data_dir + 'training_data_stopwords.txt', 'r')\n",
    "for line in read_handle:\n",
    "    line = line.replace('\\n', '').split(',')\n",
    "    training_data.append(line)\n",
    "print(len(training_data))\n",
    "\n",
    "testing_data = []\n",
    "read_handle = open(data_dir + 'testing_data_stopwords.txt', 'r')\n",
    "for line in read_handle:\n",
    "    line = line.replace('\\n', '').split(',')\n",
    "    testing_data.append(line)\n",
    "print(len(testing_data))\n",
    "read_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add 'unk' token to the vocabulary\n",
    "Replace rare words by the 'unk' token to handle out-of-vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_data)\n",
    "vocab_count = defaultdict(lambda:0)\n",
    "for sentence in training_data:\n",
    "    for word in sentence[1:-1]:\n",
    "        vocab_count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 50 rare words (count == 1), which is about 2% of the vocab count, to replace by '<unk>'\n",
    "count = 0\n",
    "rare_word_list = []\n",
    "for word in vocab_count:\n",
    "    if vocab_count[word] == 1:\n",
    "        rare_word_list.append(word)\n",
    "        count += 1\n",
    "        if count == 50:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update training data with sentences that have the 'unk' token\n",
    "training_data_with_unk = []\n",
    "for sentence in training_data:\n",
    "    sentence_with_unk = []\n",
    "    for word in sentence:\n",
    "        if word in rare_word_list:\n",
    "            sentence_with_unk.append('<unk>')\n",
    "        else:\n",
    "            sentence_with_unk.append(word)\n",
    "    training_data_with_unk.append(sentence_with_unk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unidirectional n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to count ngrams\n",
    "def ngram_count(tokenized_steps: list, n: int):\n",
    "    if n == 1:\n",
    "        # Create unigram\n",
    "        vocab = set()\n",
    "        all_words = list()\n",
    "        unigram = {}\n",
    "        \n",
    "        for line in tokenized_steps:\n",
    "            tokens = line\n",
    "            \n",
    "            # Discover new word\n",
    "            for token in tokens:\n",
    "                if token not in vocab:\n",
    "                    vocab.add(token)\n",
    "                all_words.append(token)\n",
    "                token = tuple([token])\n",
    "                if token not in unigram:\n",
    "                    unigram[token] = 1\n",
    "                else:\n",
    "                    unigram[token] += 1  \n",
    "        return [all_words, vocab, unigram]\n",
    "        \n",
    "    elif n == 2:\n",
    "        # Create bigram and unigram\n",
    "        vocab = set()\n",
    "        all_words = list()\n",
    "        unigram = {}\n",
    "        bigram = {}\n",
    "        \n",
    "        for line in tokenized_steps:\n",
    "            tokens = line\n",
    "            bigrams = list(ngrams(tokens, n=2))\n",
    "\n",
    "            # Discover new word\n",
    "            for token in tokens:\n",
    "                if token not in vocab:\n",
    "                    vocab.add(token)\n",
    "                all_words.append(token)\n",
    "                token = tuple([token])\n",
    "                if token not in unigram:\n",
    "                    unigram[token] = 1\n",
    "                else:\n",
    "                    unigram[token] += 1\n",
    "\n",
    "            for each_bigram in bigrams:\n",
    "                if each_bigram not in bigram:\n",
    "                    bigram[each_bigram] = 1\n",
    "                else:\n",
    "                    bigram[each_bigram] += 1\n",
    "        \n",
    "        return [all_words, vocab, unigram, bigram]\n",
    "\n",
    "    elif n == 3:\n",
    "        # Create trigram, bigram, and unigram\n",
    "        vocab = set()\n",
    "        all_words = list()\n",
    "        unigram = {}\n",
    "        bigram = {}\n",
    "        trigram = {}\n",
    "        \n",
    "        for line in tokenized_steps:\n",
    "            tokens = line \n",
    "            bigrams = list(ngrams(tokens, n=2))\n",
    "            trigrams = list(ngrams(tokens, n=3))\n",
    "\n",
    "            # Discover new word\n",
    "            for token in tokens:\n",
    "                if token not in vocab:\n",
    "                    vocab.add(token)\n",
    "                all_words.append(token)\n",
    "                token = tuple([token])\n",
    "                if token not in unigram:\n",
    "                    unigram[token ] = 1\n",
    "                else:\n",
    "                    unigram[token] += 1\n",
    "\n",
    "            for each_bigram in bigrams:\n",
    "                if each_bigram not in bigram:\n",
    "                    bigram[each_bigram] = 1\n",
    "                else:\n",
    "                    bigram[each_bigram] += 1\n",
    "            for each_trigram in trigrams:\n",
    "                if each_trigram not in trigram:\n",
    "                    trigram[each_trigram] = 1\n",
    "                else:\n",
    "                    trigram[each_trigram] += 1\n",
    "                    \n",
    "        return [all_words, vocab, unigram, bigram, trigram]\n",
    "\n",
    "    elif n == 4:\n",
    "        # Create fourgram, trigram, bigram, and unigram\n",
    "        vocab = set()\n",
    "        all_words = list()\n",
    "        unigram = {}\n",
    "        bigram = {}\n",
    "        trigram = {}\n",
    "        fourgram = {}\n",
    "        \n",
    "        for line in tokenized_steps:\n",
    "            tokens = line \n",
    "            bigrams = list(ngrams(tokens, n=2))\n",
    "            trigrams = list(ngrams(tokens, n=3))\n",
    "            fourgrams = list(ngrams(tokens, n=4))\n",
    "\n",
    "            # Discover new word\n",
    "            for token in tokens:\n",
    "                if token not in vocab:\n",
    "                    vocab.add(token)\n",
    "                all_words.append(token)\n",
    "                token = tuple([token])\n",
    "                if token not in unigram:\n",
    "                    unigram[token ] = 1\n",
    "                else:\n",
    "                    unigram[token] += 1\n",
    "\n",
    "            for each_bigram in bigrams:\n",
    "                if each_bigram not in bigram:\n",
    "                    bigram[each_bigram] = 1\n",
    "                else:\n",
    "                    bigram[each_bigram] += 1\n",
    "                    \n",
    "            for each_trigram in trigrams:\n",
    "                if each_trigram not in trigram:\n",
    "                    trigram[each_trigram] = 1\n",
    "                else:\n",
    "                    trigram[each_trigram] += 1\n",
    "        \n",
    "            for each_fourgram in fourgrams:\n",
    "                if each_fourgram not in fourgram:\n",
    "                    fourgram[each_fourgram] = 1\n",
    "                else:\n",
    "                    fourgram[each_fourgram] += 1\n",
    "                    \n",
    "        return [all_words, vocab, unigram, bigram, trigram, fourgram]\n",
    "\n",
    "    elif n == 5:\n",
    "        # Create fivegram, fourgram, trigram, bigram, and unigram\n",
    "        vocab = set()\n",
    "        all_words = list()\n",
    "        unigram = {}\n",
    "        bigram = {}\n",
    "        trigram = {}\n",
    "        fourgram = {}\n",
    "        fivegram = {}\n",
    "        \n",
    "        for line in tokenized_steps:\n",
    "            tokens = line \n",
    "            bigrams = list(ngrams(tokens, n=2))\n",
    "            trigrams = list(ngrams(tokens, n=3))\n",
    "            fourgrams = list(ngrams(tokens, n=4))\n",
    "            fivegrams = list(ngrams(tokens, n=5))\n",
    "\n",
    "            # Discover new word\n",
    "            for token in tokens:\n",
    "                if token not in vocab:\n",
    "                    vocab.add(token)\n",
    "                all_words.append(token)\n",
    "                token = tuple([token])\n",
    "                if token not in unigram:\n",
    "                    unigram[token ] = 1\n",
    "                else:\n",
    "                    unigram[token] += 1\n",
    "\n",
    "            for each_bigram in bigrams:\n",
    "                if each_bigram not in bigram:\n",
    "                    bigram[each_bigram] = 1\n",
    "                else:\n",
    "                    bigram[each_bigram] += 1\n",
    "                    \n",
    "            for each_trigram in trigrams:\n",
    "                if each_trigram not in trigram:\n",
    "                    trigram[each_trigram] = 1\n",
    "                else:\n",
    "                    trigram[each_trigram] += 1\n",
    "        \n",
    "            for each_fourgram in fourgrams:\n",
    "                if each_fourgram not in fourgram:\n",
    "                    fourgram[each_fourgram] = 1\n",
    "                else:\n",
    "                    fourgram[each_fourgram] += 1  \n",
    "            for each_fivegram in fivegrams:\n",
    "                if each_fivegram not in fivegram:\n",
    "                    fivegram[each_fivegram] = 1\n",
    "                else:\n",
    "                    fivegram[each_fivegram] += 1\n",
    "\n",
    "        return [all_words, vocab, unigram, bigram, trigram, fourgram, fivegram]\n",
    "    else:\n",
    "        print(\"Error! Provide a valid value for n.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to estimate n-grams' probabilities\n",
    "def get_prob_unigram(word, count_all_tokens):\n",
    "    try:\n",
    "        return (unigram[tuple([word])]) / (count_all_tokens)\n",
    "    except:\n",
    "        # If word does not exist in vocab, return estimate for the 'unk' token\n",
    "        return (unigram[('<unk>',)]) / (count_all_tokens)\n",
    "    \n",
    "def get_prob_bigram(words):\n",
    "    try:\n",
    "        return (bigram[words]) / (unigram[tuple([words[0]])])\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def get_prob_trigram(words):\n",
    "    try:\n",
    "        return (trigram[words]) / (bigram[words[:2]])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_prob_fourgram(words):\n",
    "    try:\n",
    "        return (fourgram[words]) / (trigram[words[:3]])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_prob_fivegram(words):\n",
    "    try:\n",
    "        return (fivegram[words]) / (fourgram[words[:4]])\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get the probability of a target word given the context words\n",
    "def get_probability_word(context_words, target_word, count_all_tokens, unique_vocab):\n",
    "    \"\"\" Get probability of a 'target_word' given the context words 'context_words'.\n",
    "    Use back-off: when a n-gram is not found (returned zero probability),\n",
    "    uses (n-1)-gram, and so on.\n",
    "    \n",
    "    \"\"\"\n",
    "    unique_vocab_len = len(unique_vocab)\n",
    "    # Uses 5-gram\n",
    "    if len(context_words) == 4:\n",
    "        p5 = get_prob_fivegram((context_words[-4], context_words[-3], context_words[-2], context_words[-1], target_word))\n",
    "        # If p5 == 0, 5-gram never occurred, try 4-gram\n",
    "        if p5 == 0:\n",
    "            p5 = 0.4*get_prob_fourgram((context_words[-3], context_words[-2], context_words[-1], target_word))\n",
    "            # If p4 == 0, 4-gram never occurred, try 3-gram\n",
    "            if p5 == 0:\n",
    "                p5 = 0.4*get_prob_trigram((context_words[-2], context_words[-1], target_word))\n",
    "                # If p3 == 0, 3-gram never occurred, try bigram\n",
    "                if p5 == 0:\n",
    "                    p5 = 0.4*get_prob_bigram((context_words[-1], target_word))\n",
    "                    # If p2 == 0, bigram never occurred, try unigram\n",
    "                    if p5 == 0:\n",
    "                        p5 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return p5\n",
    "    \n",
    "    # Uses 4-gram\n",
    "    elif len(context_words) == 3:\n",
    "        p4 = get_prob_fourgram((context_words[-3], context_words[-2], context_words[-1], target_word))\n",
    "        # If p4 == 0, 4-gram never occurred, try 3-gram\n",
    "        if p4 == 0:\n",
    "            p4 = 0.4*get_prob_trigram((context_words[-2], context_words[-1], target_word))\n",
    "            # If p4 == 0, 3-gram never occurred, try bigram\n",
    "            if p4 == 0:\n",
    "                p4 = 0.4*get_prob_bigram((context_words[-1], target_word))\n",
    "                # If p4 == 0, bigram never occurred, try unigram\n",
    "                if p4 == 0:      \n",
    "                    p4 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return p4\n",
    "    \n",
    "    # Uses trigram\n",
    "    elif len(context_words) == 2:\n",
    "        p3 = get_prob_trigram((context_words[-2], context_words[-1], target_word))\n",
    "        # If p3 == 0, 3-gram never occurred, try bigram\n",
    "        if p3 == 0:\n",
    "            p3 = 0.4*get_prob_bigram((context_words[-1], target_word))\n",
    "            # If p3 == 0, bigram never occurred, try unigram\n",
    "            if p3 == 0:\n",
    "                p3 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return p3\n",
    "    \n",
    "    # Uses bigram\n",
    "    elif len(context_words) == 1:\n",
    "        p2 = get_prob_bigram((context_words[-1], target_word))\n",
    "        # If p2 == 0, bigram never occurred, try unigram\n",
    "        if p2 == 0:\n",
    "            p2 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return p2\n",
    "    \n",
    "    # Uses unigram\n",
    "    else:\n",
    "        p1 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence: list, ngram_counts: dict, count_all_tokens: int, unique_vocab_len: int, unique_vocab: list):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a sentence\n",
    "\n",
    "    Args:\n",
    "       sentence: List of tokens/words\n",
    "       n_gram_counts: Dictionary of counts of n-grams\n",
    "       count_all_tokens: Int with total count of all tokens in training corpus\n",
    "       unique_vocab_len: Int with size of vocabulary of training corpus\n",
    "       unique_vocab: List of unique words in training corpus\n",
    "\n",
    "    Returns:\n",
    "       Perplexity score\n",
    "    \"\"\"\n",
    "    # length of n-grams (e.g., when n = 5, perplexity is compute with 5-grams, unless less than 5 words are available, then the model backs off)\n",
    "    n = len(list(ngram_counts.keys())[0])\n",
    "\n",
    "    # Cast the sentence from a list to a tuple\n",
    "    sentence = tuple(sentence)\n",
    "\n",
    "    # length of sentence\n",
    "    N = len(sentence)\n",
    "\n",
    "    # Cumulative product of probabilities computed by the n-grams\n",
    "    cumulative_product = 1.0\n",
    "\n",
    "    # Next, iterate through each word, except first ([START] token) and last ([END] token) indices\n",
    "    # Unigram\n",
    "    if n == 1:\n",
    "        for t in range(1, N-1):\n",
    "\n",
    "            # Get the n-gram preceding the word at position t\n",
    "            ngram = ()\n",
    "\n",
    "            # Get the word at position t\n",
    "            word = sentence[t]\n",
    "\n",
    "            # Estimate the probability of the word given the n-gram\n",
    "            probability = get_probability_word(ngram, word, count_all_tokens, unique_vocab) \n",
    "\n",
    "            # Update the product of the probabilities\n",
    "            cumulative_product *= 1/probability\n",
    "        \n",
    "        \n",
    "    elif n == 2:\n",
    "        for t in range(1, N-1):\n",
    "\n",
    "            # Get the n-gram preceding the word at position t\n",
    "            ngram = sentence[t-1:t]\n",
    "\n",
    "            # Get the word at position t\n",
    "            word = sentence[t]\n",
    "\n",
    "            # Estimate the probability of the word given the n-gram\n",
    "            probability = get_probability_word(ngram, word, count_all_tokens, unique_vocab) \n",
    "\n",
    "            # Update the product of the probabilities\n",
    "            cumulative_product *= 1/probability \n",
    "\n",
    "    elif n == 3:\n",
    "        # Get bigram first\n",
    "        t = 1\n",
    "        ngram = sentence[t-1:t]\n",
    "        word = sentence[t]\n",
    "        # Estimate the probability of the word given the n-gram\n",
    "        probability = get_probability_word(ngram, word, count_all_tokens, unique_vocab) \n",
    "        # Update the product of the probabilities\n",
    "        cumulative_product *= 1/probability \n",
    "        \n",
    "        for t in range(2, N-1):\n",
    "\n",
    "            # Get the n-gram preceding the word at position t\n",
    "            ngram = sentence[t-2:t]\n",
    "\n",
    "            # Get the word at position t\n",
    "            word = sentence[t]\n",
    "\n",
    "            # Estimate the probability of the word given the n-gram\n",
    "            probability = get_probability_word(ngram, word, count_all_tokens, unique_vocab) \n",
    "\n",
    "            # Update the product of the probabilities\n",
    "            cumulative_product *= 1/probability \n",
    "         \n",
    "    elif n == 4:\n",
    "        # Get bigram and trigram first\n",
    "        t = 1\n",
    "        ngram = sentence[t-1:t]\n",
    "        word = sentence[t]\n",
    "        # Estimate the probability of the word given the n-gram\n",
    "        probability = get_probability_word(ngram, word, count_all_tokens, unique_vocab) \n",
    "        # Update the product of the probabilities\n",
    "        cumulative_product *= 1/probability \n",
    "        \n",
    "        t = 2\n",
    "        ngram = sentence[t-2:t]\n",
    "        word = sentence[t]\n",
    "        # Estimate the probability of the word given the n-gram\n",
    "        probability = get_probability_word(ngram, word, count_all_tokens, unique_vocab) \n",
    "        # Update the product of the probabilities\n",
    "        cumulative_product *= 1/probability \n",
    "        \n",
    "        for t in range(3, N-1):\n",
    "\n",
    "            # Get the n-gram preceding the word at position t\n",
    "            ngram = sentence[t-3:t]\n",
    "\n",
    "            # Get the word at position t\n",
    "            word = sentence[t]\n",
    "\n",
    "            # Estimate the probability of the word given the n-gram\n",
    "            probability = get_probability_word(ngram, word, count_all_tokens, unique_vocab) \n",
    "\n",
    "            # Update the product of the probabilities\n",
    "            cumulative_product *= 1/probability \n",
    "            \n",
    "    elif n == 5:\n",
    "        # Get bigram, trigram, and fourgram first\n",
    "        t = 1\n",
    "        ngram = sentence[t-1:t]\n",
    "        word = sentence[t]\n",
    "        # Estimate the probability of the word given the n-gram\n",
    "        probability = get_probability_word(ngram, word, count_all_tokens, unique_vocab) \n",
    "        # Update the product of the probabilities\n",
    "        cumulative_product *= 1/probability \n",
    "        \n",
    "        t = 2\n",
    "        ngram = sentence[t-2:t]\n",
    "        word = sentence[t]\n",
    "        # Estimate the probability of the word given the n-gram\n",
    "        probability = get_probability_word(ngram, word, count_all_tokens, unique_vocab) \n",
    "        # Update the product of the probabilities\n",
    "        cumulative_product *= 1/probability  \n",
    "\n",
    "        t = 3\n",
    "        ngram = sentence[t-3:t]\n",
    "        word = sentence[t]\n",
    "        # Estimate the probability of the word given the n-gram\n",
    "        probability = get_probability_word(ngram, word, count_all_tokens, unique_vocab) \n",
    "        # Update the product of the probabilities\n",
    "        cumulative_product *= 1/probability \n",
    "        \n",
    "        for t in range(4, N-1):\n",
    "\n",
    "            # Get the n-gram preceding the word at position t\n",
    "            ngram = sentence[t-4:t]\n",
    "\n",
    "            # Get the word at position t\n",
    "            word = sentence[t]\n",
    "\n",
    "            # Estimate the probability of the word given the n-gram\n",
    "            probability = get_probability_word(ngram, word, count_all_tokens, unique_vocab) \n",
    "\n",
    "            # Update the product of the probabilities\n",
    "            cumulative_product *= 1/probability \n",
    "\n",
    "    # Take the Nth root of the product to compute perplexity\n",
    "    perplexity = cumulative_product**(1/N)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build n-gram models with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build n-grams using 5-gram\n",
    "ngram_models = ngram_count(training_data_with_unk, 5)\n",
    "all_words = ngram_models[0]\n",
    "unique_vocab = ngram_models[1]\n",
    "unigram = ngram_models[2]\n",
    "bigram = ngram_models[3]\n",
    "trigram = ngram_models[4]\n",
    "fourgram = ngram_models[5]\n",
    "fivegram = ngram_models[6]\n",
    "count_all_tokens = len(all_words)\n",
    "unique_vocab_len = len(unique_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of words in the training corpus: \", count_all_tokens)\n",
    "print(\"Total number of unique words in the training corpus: \", unique_vocab_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute perplexity score for training data with the unidirectional n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pandas df to store the compute perplexities\n",
    "perplexity_df = pd.DataFrame(columns = ['perplexity_score', 'ngram', 'group'])\n",
    "index_to_add = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through training data to compute average perplexity\n",
    "for train in training_data_with_unk:\n",
    "    \n",
    "    # Using unigram\n",
    "    perplexity_uni = calculate_perplexity(train, unigram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "    perplexity_df.loc[index_to_add] = [perplexity_uni, 'Unigram', 'Training Data']\n",
    "    index_to_add += 1\n",
    "    \n",
    "    # Using bigram\n",
    "    perplexity_bi = calculate_perplexity(train, bigram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "    perplexity_df.loc[index_to_add] = [perplexity_bi, 'Bigram', 'Training Data']\n",
    "    index_to_add += 1\n",
    "    \n",
    "    # Using trigram\n",
    "    if len(train) >= 3:\n",
    "        perplexity_tri = calculate_perplexity(train, trigram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "        perplexity_df.loc[index_to_add] = [perplexity_tri, 'Trigram', 'Training Data']\n",
    "        index_to_add += 1\n",
    "    \n",
    "    else:\n",
    "        perplexity_tri = None\n",
    "    \n",
    "    # Using fourgram\n",
    "    if len(train) >= 4:\n",
    "        perplexity_four = calculate_perplexity(train, fourgram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "        perplexity_df.loc[index_to_add] = [perplexity_four, '4-gram', 'Training Data']\n",
    "        index_to_add += 1\n",
    "\n",
    "    else:\n",
    "        perplexity_four = None\n",
    "        \n",
    "    # Using fivegram\n",
    "    if len(train) >= 5:\n",
    "        perplexity_five = calculate_perplexity(train, fivegram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "        perplexity_df.loc[index_to_add] = [perplexity_five, '5-gram', 'Training Data']\n",
    "        index_to_add += 1\n",
    "        \n",
    "    else:\n",
    "        perplexity_five = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute perplexity score for testing data with the unidreictional n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through testing data to compute average perplexity\n",
    "perplexity_uni_test_list = []\n",
    "perplexity_bi_test_list = []\n",
    "perplexity_tri_test_list = []\n",
    "perplexity_four_test_list = []\n",
    "perplexity_five_test_list = []\n",
    "\n",
    "for test in testing_data:\n",
    "        \n",
    "    # Using unigram\n",
    "    perplexity_uni = calculate_perplexity(test, unigram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "    perplexity_df.loc[index_to_add] = [perplexity_uni, 'Unigram', 'Testing Data']\n",
    "    index_to_add += 1\n",
    "        \n",
    "    # Using bigram\n",
    "    perplexity_bi = calculate_perplexity(test, bigram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "    perplexity_df.loc[index_to_add] = [perplexity_bi, 'Bigram', 'Testing Data']\n",
    "    index_to_add += 1\n",
    "        \n",
    "    # Using trigram\n",
    "    if len(test) >= 3:\n",
    "        perplexity_tri = calculate_perplexity(test, trigram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "        perplexity_df.loc[index_to_add] = [perplexity_tri, 'Trigram', 'Testing Data']\n",
    "        index_to_add += 1\n",
    "    \n",
    "    else:\n",
    "        perplexity_tri = None\n",
    "    \n",
    "    # Using fourgram\n",
    "    if len(test) >= 4:\n",
    "        perplexity_four = calculate_perplexity(test, fourgram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "        perplexity_df.loc[index_to_add] = [perplexity_four, '4-gram', 'Testing Data']\n",
    "        index_to_add += 1\n",
    "    else:\n",
    "        perplexity_four = None\n",
    "        \n",
    "    # Using fivegram\n",
    "    if len(test) >= 5:\n",
    "        perplexity_five = calculate_perplexity(test, fivegram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "        perplexity_df.loc[index_to_add] = [perplexity_five, '5-gram', 'Testing Data']\n",
    "        index_to_add += 1\n",
    "    else:\n",
    "        perplexity_five = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers if necessary\n",
    "def remove_outliers(df):\n",
    "    temp_df = df.loc[(df['ngram'] == 'Unigram') & (df['group'] == 'Training Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    \n",
    "    filter = (df['ngram'] == 'Unigram') & (df['group'] == 'Training Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_1 = df.loc[filter]\n",
    "    \n",
    " \n",
    "    temp_df = df.loc[(df['ngram'] == 'Unigram') & (df['group'] == 'Testing Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == 'Unigram') & (df['group'] == 'Testing Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_2 = df.loc[filter]\n",
    "    \n",
    "    \n",
    "    temp_df = df.loc[(df['ngram'] == 'Bigram') & (df['group'] == 'Training Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == 'Bigram') & (df['group'] == 'Training Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_3 = df.loc[filter]    \n",
    "    \n",
    " \n",
    "    temp_df = df.loc[(df['ngram'] == 'Bigram') & (df['group'] == 'Testing Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == 'Bigram') & (df['group'] == 'Testing Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_4 = df.loc[filter]\n",
    "    \n",
    "    \n",
    "    temp_df = df.loc[(df['ngram'] == 'Trigram') & (df['group'] == 'Training Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == 'Trigram') & (df['group'] == 'Training Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_5 = df.loc[filter]    \n",
    "    \n",
    " \n",
    "    temp_df = df.loc[(df['ngram'] == 'Trigram') & (df['group'] == 'Testing Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == 'Trigram') & (df['group'] == 'Testing Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_6 = df.loc[filter]\n",
    "    \n",
    "    \n",
    "    temp_df = df.loc[(df['ngram'] == '4-gram') & (df['group'] == 'Training Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == '4-gram') & (df['group'] == 'Training Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_7 = df.loc[filter]    \n",
    "    \n",
    " \n",
    "    temp_df = df.loc[(df['ngram'] == '4-gram') & (df['group'] == 'Testing Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == '4-gram') & (df['group'] == 'Testing Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_8 = df.loc[filter]    \n",
    "    \n",
    "    \n",
    "    temp_df = df.loc[(df['ngram'] == '5-gram') & (df['group'] == 'Training Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == '5-gram') & (df['group'] == 'Training Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_9 = df.loc[filter]    \n",
    "    \n",
    " \n",
    "    temp_df = df.loc[(df['ngram'] == '5-gram') & (df['group'] == 'Testing Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == '5-gram') & (df['group'] == 'Testing Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_10 = df.loc[filter]\n",
    "    \n",
    "    main_df = df_1.append([df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10])\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Perplexity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for quick visualization\n",
    "plotly.offline.init_notebook_mode()\n",
    "pio.renderers.default = 'iframe' # or 'notebook' or 'colab' or 'jupyterlab'\n",
    "fig = go.Figure()\n",
    "\n",
    "perplexity_outlier_df = remove_outliers(perplexity_df)\n",
    "fig = px.box(perplexity_outlier_df, x=\"ngram\", y=\"perplexity_score\", color=\"group\", points=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Unidirectional N-grams\",\n",
    "    title_x=0.5,\n",
    "    xaxis_title=\"N-gram order\",\n",
    "    yaxis_title=\"Perplexity metric\",\n",
    "    legend_title=\"Dataset\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"RebeccaPurple\"\n",
    "    )\n",
    ")\n",
    "fig.show()\n",
    "# fig.write_image(\"images/unidir_ngram_perplexity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute median perplexities\n",
    "median_unigram = st.median(perplexity_df[(perplexity_df['ngram'] == 'Unigram') & (perplexity_df['group'] == 'Testing Data')]['perplexity_score'].tolist())\n",
    "median_bigram = st.median(perplexity_df[(perplexity_df['ngram'] == 'Bigram') & (perplexity_df['group'] == 'Testing Data')]['perplexity_score'].tolist())\n",
    "median_trigram = st.median(perplexity_df[(perplexity_df['ngram'] == 'Trigram') & (perplexity_df['group'] == 'Testing Data')]['perplexity_score'].tolist())\n",
    "median_fourgram = st.median(perplexity_df[(perplexity_df['ngram'] == '4-gram') & (perplexity_df['group'] == 'Testing Data')]['perplexity_score'].tolist())\n",
    "median_fivegram = st.median(perplexity_df[(perplexity_df['ngram'] == '5-gram') & (perplexity_df['group'] == 'Testing Data')]['perplexity_score'].tolist())\n",
    "print(median_unigram)\n",
    "print(median_bigram)\n",
    "print(median_trigram)\n",
    "print(median_fourgram)\n",
    "print(median_fivegram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes as CSV to be read by R and generate plots for the paper\n",
    "perplexity_df.to_csv('uni_ngrams/perplexity_ngram.csv', index=False)\n",
    "perplexity_outlier_df.to_csv('uni_ngrams/perplexity_ngram_outlier.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bidirectional n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to estimate bidirectional n-grams' probabilities\n",
    "def get_prob_unigram(word, count_all_tokens):\n",
    "    try:\n",
    "        return (unigram[tuple([word])]) / (count_all_tokens)\n",
    "    except:\n",
    "        # If word does not exist in vocab, return estimate for the 'unk' token\n",
    "        return (unigram[('<unk>',)]) / (count_all_tokens)\n",
    "\n",
    "\n",
    "def get_prob_bigram_before(words):\n",
    "    try:\n",
    "        return (bigram[words]) / (unigram[tuple([words[0]])])\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def get_prob_bigram_after(words):\n",
    "    try:\n",
    "        return (bigram[words]) / (unigram[tuple([words[-1]])])\n",
    "    except:\n",
    "        return 0    \n",
    "\n",
    "\n",
    "def get_prob_trigram_before(words):\n",
    "    try:\n",
    "        return (trigram[words]) / (bigram[words[:2]])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_prob_trigram_after(words):\n",
    "    try:\n",
    "        return (trigram[words]) / (bigram[words[-2:]])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_prob_fourgram_before(words):\n",
    "    try:\n",
    "        return (fourgram[words]) / (trigram[words[:3]])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_prob_fourgram_after(words):\n",
    "    try:\n",
    "        return (fourgram[words]) / (trigram[words[-3:]])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_prob_fivegram_before(words):\n",
    "    try:\n",
    "        return (fivegram[words]) / (fourgram[words[:4]])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_prob_fivegram_after(words):\n",
    "    try:\n",
    "        return (fivegram[words]) / (fourgram[words[-4:]])\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get the probability of a target word given the context words to the left (before) of the target word\n",
    "def get_probability_word_before(words_before, target_word, len_words_before, count_all_tokens, unique_vocab_len):\n",
    "    \"\"\"\" Get probability for 'target_word' using only context words that come before the word.\n",
    "    Use back-off if n-gram does not exist.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Uses 5-gram\n",
    "    if len_words_before == 4:\n",
    "        p5 = get_prob_fivegram_before((words_before[-4], words_before[-3], words_before[-2], words_before[-1], target_word))\n",
    "        # If p5 == 0, 5-gram never occurred, try 4-gram\n",
    "        if p5 == 0:\n",
    "            p5 = 0.4*get_prob_fourgram_before((words_before[-3], words_before[-2], words_before[-1], target_word))\n",
    "            # If p4 == 0, 4-gram never occurred, try 3-gram\n",
    "            if p5 == 0:\n",
    "                p5 = 0.4*get_prob_trigram_before((words_before[-2], words_before[-1], target_word))\n",
    "                # If p3 == 0, 3-gram never occurred, try bigram\n",
    "                if p5 == 0:\n",
    "                    p5 = 0.4*get_prob_bigram_before((words_before[-1], target_word))\n",
    "                    # If p2 == 0, bigram never occurred, try unigram\n",
    "                    if p5 == 0:\n",
    "                        p5 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return p5\n",
    "    \n",
    "    # Uses 4-gram\n",
    "    elif len_words_before == 3:\n",
    "        p4 = get_prob_fourgram_before((words_before[-3], words_before[-2], words_before[-1], target_word))\n",
    "        # If p4 == 0, 4-gram never occurred, try 3-gram\n",
    "        if p4 == 0:\n",
    "            p4 = 0.4*get_prob_trigram_before((words_before[-2], words_before[-1], target_word))\n",
    "            # If p4 == 0, 3-gram never occurred, try bigram\n",
    "            if p4 == 0:\n",
    "                p4 = 0.4*get_prob_bigram_before((words_before[-1], target_word))\n",
    "                # If p4 == 0, bigram never occurred, try unigram\n",
    "                if p4 == 0:   \n",
    "                    p4 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return p4\n",
    "    \n",
    "    # Uses trigram\n",
    "    elif len_words_before == 2:\n",
    "        p3 = get_prob_trigram_before((words_before[-2], words_before[-1], target_word))\n",
    "        # If p3 == 0, 3-gram never occurred, try bigram\n",
    "        if p3 == 0:\n",
    "            p3 = 0.4*get_prob_bigram_before((words_before[-1], target_word))\n",
    "            # If p3 == 0, bigram never occurred, try unigram\n",
    "            if p3 == 0:\n",
    "                p3 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return p3\n",
    "    \n",
    "    # Uses bigram\n",
    "    elif len_words_before == 1:\n",
    "        p2 = get_prob_bigram_before((words_before[-1], target_word))\n",
    "        # If p2 == 0, bigram never occurred, try unigram\n",
    "        if p2 == 0:\n",
    "            p2 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return p2\n",
    "    \n",
    "    # Uses unigram\n",
    "    else:\n",
    "        p1 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get the probability of a target word given the context words to the right (after) of the target word\n",
    "def get_probability_word_after(words_after, target_word, len_words_after, count_all_tokens, unique_vocab_len):\n",
    "    \"\"\"\" Get probability for 'target_word' using only context words that come after the word.\n",
    "    Use back-off if n-gram does not exist.\n",
    "    \n",
    "    \"\"\"  \n",
    "    # Uses 5-gram\n",
    "    if len_words_after == 4:\n",
    "        p5 = get_prob_fivegram_after((target_word, words_after[0], words_after[1], words_after[2], words_after[3]))\n",
    "        # If p5 == 0, 5-gram never occurred, try 4-gram\n",
    "        if p5 == 0:\n",
    "            p5 = 0.4*get_prob_fourgram_after((target_word, words_after[0], words_after[1], words_after[2]))\n",
    "            # If p4 == 0, 4-gram never occurred, try 3-gram\n",
    "            if p5 == 0:\n",
    "                p5 = 0.4*get_prob_trigram_after((target_word, words_after[0], words_after[1]))\n",
    "                # If p3 == 0, 3-gram never occurred, try bigram\n",
    "                if p5 == 0:\n",
    "                    p5 = 0.4*get_prob_bigram_after((target_word, words_after[0]))\n",
    "                    # If p2 == 0, bigram never occurred, try unigram\n",
    "                    if p5 == 0:\n",
    "                        p5 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return p5\n",
    "    \n",
    "    # Uses 4-gram\n",
    "    elif len_words_after == 3:\n",
    "        p4 = get_prob_fourgram_after((target_word, words_after[0], words_after[1], words_after[2]))\n",
    "        # If p4 == 0, 4-gram never occurred, try 3-gram\n",
    "        if p4 == 0:\n",
    "            p4 = 0.4*get_prob_trigram_after((target_word, words_after[0], words_after[1]))\n",
    "            # If p4 == 0, 3-gram never occurred, try bigram\n",
    "            if p4 == 0:\n",
    "                p4 = 0.4*get_prob_bigram_after((target_word, words_after[0]))\n",
    "                # If p4 == 0, bigram never occurred, try unigram\n",
    "                if p4 == 0:      \n",
    "                    p4 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return p4\n",
    "    \n",
    "    # Uses trigram\n",
    "    elif len_words_after == 2:\n",
    "        p3 = get_prob_trigram_after((target_word, words_after[0], words_after[1]))\n",
    "        # If p3 == 0, 3-gram never occurred, try bigram\n",
    "        if p3 == 0:\n",
    "            p3 = 0.4*get_prob_bigram_after((target_word, words_after[0]))\n",
    "            # If p3 == 0, bigram never occurred, try unigram\n",
    "            if p3 == 0:\n",
    "                p3 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return p3\n",
    "    \n",
    "    # Uses bigram\n",
    "    elif len_words_after == 1:\n",
    "        p2 = get_prob_bigram_after((target_word, words_after[0]))\n",
    "        # If p2 == 0, bigram never occurred, try unigram\n",
    "        if p2 == 0:\n",
    "            p2 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return p2\n",
    "    \n",
    "    # Uses unigram\n",
    "    else:\n",
    "        p1 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get the probability of a target word given the context words to the left (before) and right (after) of the target word - bidirectional\n",
    "def get_probability_word_bidir(words_before, words_after, target_word, count_all_tokens, unique_vocab):\n",
    "    \"\"\" Get probability of a 'target_word' given the context words 'words_before' and 'words_after'.\n",
    "    \n",
    "    \"\"\"\n",
    "    unique_vocab_len = len(unique_vocab)    \n",
    "    len_words_before = len(words_before)\n",
    "    len_words_after = len(words_after)\n",
    "    \n",
    "    prob_before = get_probability_word_before(words_before, target_word, len_words_before, count_all_tokens, unique_vocab_len)\n",
    "    prob_after = get_probability_word_after(words_after, target_word, len_words_after, count_all_tokens, unique_vocab_len)\n",
    "    prob_final = (prob_before + prob_after)/2.0\n",
    "    \n",
    "    return prob_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity_bidir(sentence: list,\n",
    "                         ngram_counts: dict,\n",
    "                         count_all_tokens: int,\n",
    "                         unique_vocab_len: int,\n",
    "                         unique_vocab: list):\n",
    "    \n",
    "    n = len(list(ngram_counts.keys())[0])\n",
    "\n",
    "    # Cast the sentence from a list to a tuple\n",
    "    sentence = tuple(sentence)\n",
    "\n",
    "    # length of sentence\n",
    "    N = len(sentence)\n",
    "\n",
    "    cumulative_product = 1.0\n",
    "    \n",
    "    # Computing perplexity - indices from 1 to N-1 to exclude [START] and [END] tokens\n",
    "    for t in range(1, N-1): \n",
    "        if (t-n) < 0:\n",
    "            previous_ngram = sentence[:t]\n",
    "        else:\n",
    "            previous_ngram = sentence[t-(n-1):t]\n",
    "\n",
    "        word = sentence[t]\n",
    "\n",
    "        subsequent_ngram = sentence[t+1:t+n]\n",
    "\n",
    "        # Estimate the probability of the word given the n-gram before and after\n",
    "        probability = get_probability_word_bidir(previous_ngram, subsequent_ngram, word, count_all_tokens, unique_vocab) \n",
    "\n",
    "        # Update the product of the probabilities\n",
    "        cumulative_product *= 1/probability     \n",
    "\n",
    "    # Take the Nth root of the product to obtain perplexity\n",
    "    perplexity = cumulative_product**(1/N)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build n-gram models with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_models = ngram_count(training_data_with_unk, 5)\n",
    "all_words = ngram_models[0]\n",
    "unique_vocab = ngram_models[1]\n",
    "unigram = ngram_models[2]\n",
    "bigram = ngram_models[3]\n",
    "trigram = ngram_models[4]\n",
    "fourgram = ngram_models[5]\n",
    "fivegram = ngram_models[6]\n",
    "\n",
    "count_all_tokens = len(all_words)\n",
    "unique_vocab_len = len(unique_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of words in the training corpus: \", count_all_tokens)\n",
    "print(\"Total number of unique words in the training corpus: \", unique_vocab_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute perplexity score with training data for bidirectional n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pandas df to store computed perplexities\n",
    "perplexity_bidir_df = pd.DataFrame(columns = ['perplexity_score', 'ngram', 'group'])\n",
    "index_to_add = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through training data to compute average perplexity\n",
    "for train in training_data_with_unk:\n",
    "    \n",
    "    # Using unigram\n",
    "    perplexity_uni = calculate_perplexity_bidir(train, unigram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "    perplexity_bidir_df.loc[index_to_add] = [perplexity_uni, 'Unigram', 'Training Data']\n",
    "    index_to_add += 1\n",
    "    \n",
    "    # Using bigram\n",
    "    perplexity_bi = calculate_perplexity_bidir(train, bigram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "    perplexity_bidir_df.loc[index_to_add] = [perplexity_bi, 'Bigram', 'Training Data']\n",
    "    index_to_add += 1\n",
    "    \n",
    "    # Using trigram\n",
    "    if len(train) >= 3:\n",
    "        perplexity_tri = calculate_perplexity_bidir(train, trigram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "        perplexity_bidir_df.loc[index_to_add] = [perplexity_tri, 'Trigram', 'Training Data']\n",
    "        index_to_add += 1\n",
    "    \n",
    "    else:\n",
    "        perplexity_tri = None\n",
    "    \n",
    "    # Using fourgram\n",
    "    if len(train) >= 4:\n",
    "        perplexity_four = calculate_perplexity_bidir(train, fourgram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "        perplexity_bidir_df.loc[index_to_add] = [perplexity_four, '4-gram', 'Training Data']\n",
    "        index_to_add += 1\n",
    "\n",
    "    else:\n",
    "        perplexity_four = None\n",
    "        \n",
    "    # Using fivegram\n",
    "    if len(train) >= 5:\n",
    "        perplexity_five = calculate_perplexity_bidir(train, fivegram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "        perplexity_bidir_df.loc[index_to_add] = [perplexity_five, '5-gram', 'Training Data']\n",
    "        index_to_add += 1\n",
    "        \n",
    "    else:\n",
    "        perplexity_five = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute perplexity score with testing data for bidirectional n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through testing data to compute average perplexity\n",
    "for test in testing_data:\n",
    "        \n",
    "    # Using unigram\n",
    "    perplexity_uni = calculate_perplexity_bidir(test, unigram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "    perplexity_bidir_df.loc[index_to_add] = [perplexity_uni, 'Unigram', 'Testing Data']\n",
    "    index_to_add += 1\n",
    "        \n",
    "    # Using bigram\n",
    "    perplexity_bi = calculate_perplexity_bidir(test, bigram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "    perplexity_bidir_df.loc[index_to_add] = [perplexity_bi, 'Bigram', 'Testing Data']\n",
    "    index_to_add += 1\n",
    "        \n",
    "    # Using trigram\n",
    "    if len(test) >= 3:\n",
    "        perplexity_tri = calculate_perplexity_bidir(test, trigram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "        perplexity_bidir_df.loc[index_to_add] = [perplexity_tri, 'Trigram', 'Testing Data']\n",
    "        index_to_add += 1\n",
    "    \n",
    "    else:\n",
    "        perplexity_tri = None\n",
    "    \n",
    "    # Using fourgram\n",
    "    if len(test) >= 4:\n",
    "        perplexity_four = calculate_perplexity_bidir(test, fourgram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "        perplexity_bidir_df.loc[index_to_add] = [perplexity_four, '4-gram', 'Testing Data']\n",
    "        index_to_add += 1\n",
    "    else:\n",
    "        perplexity_four = None\n",
    "        \n",
    "    # Using fivegram\n",
    "    if len(test) >= 5:\n",
    "        perplexity_five = calculate_perplexity_bidir(test, fivegram, count_all_tokens, unique_vocab_len, unique_vocab)\n",
    "        perplexity_bidir_df.loc[index_to_add] = [perplexity_five, '5-gram', 'Testing Data']\n",
    "        index_to_add += 1\n",
    "    else:\n",
    "        perplexity_five = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Perplexity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers if necessary\n",
    "def remove_outliers(df):\n",
    "    temp_df = df.loc[(df['ngram'] == 'Unigram') & (df['group'] == 'Training Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == 'Unigram') & (df['group'] == 'Training Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_1 = df.loc[filter]\n",
    "    \n",
    " \n",
    "    temp_df = df.loc[(df['ngram'] == 'Unigram') & (df['group'] == 'Testing Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == 'Unigram') & (df['group'] == 'Testing Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_2 = df.loc[filter]\n",
    "    \n",
    "    \n",
    "    temp_df = df.loc[(df['ngram'] == 'Bigram') & (df['group'] == 'Training Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == 'Bigram') & (df['group'] == 'Training Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_3 = df.loc[filter]    \n",
    "    \n",
    " \n",
    "    temp_df = df.loc[(df['ngram'] == 'Bigram') & (df['group'] == 'Testing Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == 'Bigram') & (df['group'] == 'Testing Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_4 = df.loc[filter]\n",
    "    \n",
    "    \n",
    "    temp_df = df.loc[(df['ngram'] == 'Trigram') & (df['group'] == 'Training Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == 'Trigram') & (df['group'] == 'Training Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_5 = df.loc[filter]    \n",
    "    \n",
    " \n",
    "    temp_df = df.loc[(df['ngram'] == 'Trigram') & (df['group'] == 'Testing Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == 'Trigram') & (df['group'] == 'Testing Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_6 = df.loc[filter]\n",
    "    \n",
    "    \n",
    "    temp_df = df.loc[(df['ngram'] == '4-gram') & (df['group'] == 'Training Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == '4-gram') & (df['group'] == 'Training Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_7 = df.loc[filter]    \n",
    "    \n",
    " \n",
    "    temp_df = df.loc[(df['ngram'] == '4-gram') & (df['group'] == 'Testing Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == '4-gram') & (df['group'] == 'Testing Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_8 = df.loc[filter]    \n",
    "    \n",
    "    \n",
    "    temp_df = df.loc[(df['ngram'] == '5-gram') & (df['group'] == 'Training Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == '5-gram') & (df['group'] == 'Training Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_9 = df.loc[filter]    \n",
    "    \n",
    " \n",
    "    temp_df = df.loc[(df['ngram'] == '5-gram') & (df['group'] == 'Testing Data')]\n",
    "    Q1 = temp_df['perplexity_score'].quantile(0.25)\n",
    "    Q3 = temp_df['perplexity_score'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df['ngram'] == '5-gram') & (df['group'] == 'Testing Data') & (df['perplexity_score'] >= Q1 - 1.5 * IQR) & (df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "    df_10 = df.loc[filter]\n",
    "    \n",
    "    main_df = df_1.append([df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10])\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot perplexities\n",
    "pio.renderers.default = 'iframe' # or 'notebook' or 'colab' or 'jupyterlab'\n",
    "fig = go.Figure()\n",
    "fig = px.box(perplexity_bidir_outlier_df, x=\"ngram\", y=\"perplexity_score\", color=\"group\", points=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Bidirectional N-grams\",\n",
    "    title_x=0.5,\n",
    "    xaxis_title=\"N-gram order\",\n",
    "    yaxis_title=\"Perplexity metric\",\n",
    "    legend_title=\"Dataset\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"RebeccaPurple\"\n",
    "    )\n",
    ")\n",
    "fig.show()\n",
    "# fig.write_image(\"images/bidir_ngram_perplexity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute median perplexities\n",
    "median_unigram = st.median(perplexity_bidir_df[(perplexity_bidir_df['ngram'] == 'Unigram') & (perplexity_bidir_df['group'] == 'Testing Data')]['perplexity_score'].tolist())\n",
    "median_bigram = st.median(perplexity_bidir_df[(perplexity_bidir_df['ngram'] == 'Bigram') & (perplexity_bidir_df['group'] == 'Testing Data')]['perplexity_score'].tolist())\n",
    "median_trigram = st.median(perplexity_bidir_df[(perplexity_bidir_df['ngram'] == 'Trigram') & (perplexity_bidir_df['group'] == 'Testing Data')]['perplexity_score'].tolist())\n",
    "median_fourgram = st.median(perplexity_bidir_df[(perplexity_bidir_df['ngram'] == '4-gram') & (perplexity_bidir_df['group'] == 'Testing Data')]['perplexity_score'].tolist())\n",
    "median_fivegram = st.median(perplexity_bidir_df[(perplexity_bidir_df['ngram'] == '5-gram') & (perplexity_bidir_df['group'] == 'Testing Data')]['perplexity_score'].tolist())\n",
    "print(median_unigram)\n",
    "print(median_bigram)\n",
    "print(median_trigram)\n",
    "print(median_fourgram)\n",
    "print(median_fivegram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes\n",
    "perplexity_bidir_df.to_csv('bidir_ngrams/perplexity_ngram_bidir.csv', index=False)\n",
    "perplexity_bidir_outlier_df.to_csv('bidir_ngrams/perplexity_ngram_bidir_outlier.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
