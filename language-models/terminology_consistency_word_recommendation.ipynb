{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology improvement analysis through language modeling\n",
    "In this notebook, we compare the following best-performing (in terms of perplexity) language models for the word recommendation task (using a recommendation system-like metric):\n",
    "\n",
    "* Statistical language model: **bidirectional 5-gram**\n",
    "* Neural language model: **Fine-tuned BERT large uncased whole word masking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requirements\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import statistics as st\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "from statistics import mean\n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "import collections\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from expects import (contain_exactly, equal, expect, have_keys)\n",
    "import attr\n",
    "from functools import partial\n",
    "from tabulate import tabulate\n",
    "\n",
    "from transformers import pipeline, BertTokenizer, BertForMaskedLM, AutoTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-processed data\n",
    "The cells below load the entire testing set. To load the 'short test step sentence' and 'long test step sentence' sets, uncomment the code with those conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'training_testing_data/with_name_objective/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "read_handle = open(data_dir + 'training_data.txt', 'r')\n",
    "for line in read_handle:\n",
    "    line = line.replace('\\n', '').split(',')\n",
    "    training_data.append(line)\n",
    "print(len(training_data))\n",
    "\n",
    "testing_data = []\n",
    "read_handle = open(data_dir + 'testing_data.txt', 'r')\n",
    "for line in read_handle:\n",
    "    line = line.replace('\\n', '').split(',')\n",
    "    testing_data.append(line)\n",
    "print(len(testing_data))\n",
    "read_handle.close()\n",
    "\n",
    "# Test steps with less than 5 words (short sentences)\n",
    "# testing_data_short = []\n",
    "# read_handle = open(data_dir + 'testing_data_stopwords.txt', 'r')\n",
    "# for line in read_handle:\n",
    "#     line = line.replace('\\n', '').split(',')\n",
    "#     if len(line) < 5:\n",
    "#         testing_data_short.append(line)\n",
    "# print(len(testing_data_short))\n",
    "# read_handle.close()\n",
    "\n",
    "# Test steps with more than 12 words (long sentences)\n",
    "# testing_data_long = []\n",
    "# read_handle = open(data_dir + 'testing_data_stopwords.txt', 'r')\n",
    "# for line in read_handle:\n",
    "#     line = line.replace('\\n', '').split(',')\n",
    "#     if len(line) > 12:\n",
    "#         testing_data_long.append(line)\n",
    "# print(len(testing_data_long))\n",
    "# read_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram: bidirectional 5-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add 'unk' token to the vocabulary of the training data to build the n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_data)\n",
    "vocab_count = defaultdict(lambda:0)\n",
    "for sentence in training_data:\n",
    "    for word in sentence[1:-1]:\n",
    "        vocab_count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 50 rare words (count == 1), which is about 2% of the vocab count, to replace by '<unk>'\n",
    "count = 0\n",
    "rare_word_list = []\n",
    "for word in vocab_count:\n",
    "    if vocab_count[word] == 1:\n",
    "        rare_word_list.append(word)\n",
    "        count += 1\n",
    "        if count == 50:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update training data with sentences that have the 'unk' token\n",
    "training_data_with_unk = []\n",
    "for sentence in training_data:\n",
    "    sentence_with_unk = []\n",
    "    for word in sentence:\n",
    "        if word in rare_word_list:\n",
    "            sentence_with_unk.append('<unk>')\n",
    "        else:\n",
    "            sentence_with_unk.append(word)\n",
    "    training_data_with_unk.append(sentence_with_unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to count ngrams\n",
    "def ngram_count(tokenized_steps: list, n: int):\n",
    "    if n == 1:\n",
    "        # Create unigram\n",
    "        vocab = set()\n",
    "        all_words = list()\n",
    "        unigram = {}\n",
    "        \n",
    "        for line in tokenized_steps:\n",
    "            tokens = line\n",
    "            \n",
    "            # Discover new word\n",
    "            for token in tokens:\n",
    "                if token not in vocab:\n",
    "                    vocab.add(token)\n",
    "                all_words.append(token)\n",
    "                token = tuple([token])\n",
    "                if token not in unigram:\n",
    "                    unigram[token] = 1\n",
    "                else:\n",
    "                    unigram[token] += 1  \n",
    "        return [all_words, vocab, unigram]\n",
    "        \n",
    "    elif n == 2:\n",
    "        # Create bigram and unigram\n",
    "        vocab = set()\n",
    "        all_words = list()\n",
    "        unigram = {}\n",
    "        bigram = {}\n",
    "        \n",
    "        for line in tokenized_steps:\n",
    "            tokens = line\n",
    "            bigrams = list(ngrams(tokens, n=2))\n",
    "\n",
    "            # Discover new word\n",
    "            for token in tokens:\n",
    "                if token not in vocab:\n",
    "                    vocab.add(token)\n",
    "                all_words.append(token)\n",
    "                token = tuple([token])\n",
    "                if token not in unigram:\n",
    "                    unigram[token] = 1\n",
    "                else:\n",
    "                    unigram[token] += 1\n",
    "\n",
    "            for each_bigram in bigrams:\n",
    "                if each_bigram not in bigram:\n",
    "                    bigram[each_bigram] = 1\n",
    "                else:\n",
    "                    bigram[each_bigram] += 1\n",
    "        \n",
    "        return [all_words, vocab, unigram, bigram]\n",
    "\n",
    "    elif n == 3:\n",
    "        # Create trigram, bigram, and unigram\n",
    "        vocab = set()\n",
    "        all_words = list()\n",
    "        unigram = {}\n",
    "        bigram = {}\n",
    "        trigram = {}\n",
    "        \n",
    "        for line in tokenized_steps:\n",
    "            tokens = line \n",
    "            bigrams = list(ngrams(tokens, n=2))\n",
    "            trigrams = list(ngrams(tokens, n=3))\n",
    "\n",
    "            # Discover new word\n",
    "            for token in tokens:\n",
    "                if token not in vocab:\n",
    "                    vocab.add(token)\n",
    "                all_words.append(token)\n",
    "                token = tuple([token])\n",
    "                if token not in unigram:\n",
    "                    unigram[token ] = 1\n",
    "                else:\n",
    "                    unigram[token] += 1\n",
    "\n",
    "            for each_bigram in bigrams:\n",
    "                if each_bigram not in bigram:\n",
    "                    bigram[each_bigram] = 1\n",
    "                else:\n",
    "                    bigram[each_bigram] += 1\n",
    "            for each_trigram in trigrams:\n",
    "                if each_trigram not in trigram:\n",
    "                    trigram[each_trigram] = 1\n",
    "                else:\n",
    "                    trigram[each_trigram] += 1\n",
    "                    \n",
    "        return [all_words, vocab, unigram, bigram, trigram]\n",
    "\n",
    "    elif n == 4:\n",
    "        # Create fourgram, trigram, bigram, and unigram\n",
    "        vocab = set()\n",
    "        all_words = list()\n",
    "        unigram = {}\n",
    "        bigram = {}\n",
    "        trigram = {}\n",
    "        fourgram = {}\n",
    "        \n",
    "        for line in tokenized_steps:\n",
    "            tokens = line \n",
    "            bigrams = list(ngrams(tokens, n=2))\n",
    "            trigrams = list(ngrams(tokens, n=3))\n",
    "            fourgrams = list(ngrams(tokens, n=4))\n",
    "\n",
    "            # Discover new word\n",
    "            for token in tokens:\n",
    "                if token not in vocab:\n",
    "                    vocab.add(token)\n",
    "                all_words.append(token)\n",
    "                token = tuple([token])\n",
    "                if token not in unigram:\n",
    "                    unigram[token ] = 1\n",
    "                else:\n",
    "                    unigram[token] += 1\n",
    "\n",
    "            for each_bigram in bigrams:\n",
    "                if each_bigram not in bigram:\n",
    "                    bigram[each_bigram] = 1\n",
    "                else:\n",
    "                    bigram[each_bigram] += 1\n",
    "                    \n",
    "            for each_trigram in trigrams:\n",
    "                if each_trigram not in trigram:\n",
    "                    trigram[each_trigram] = 1\n",
    "                else:\n",
    "                    trigram[each_trigram] += 1\n",
    "        \n",
    "            for each_fourgram in fourgrams:\n",
    "                if each_fourgram not in fourgram:\n",
    "                    fourgram[each_fourgram] = 1\n",
    "                else:\n",
    "                    fourgram[each_fourgram] += 1\n",
    "                    \n",
    "        return [all_words, vocab, unigram, bigram, trigram, fourgram]\n",
    "\n",
    "    elif n == 5:\n",
    "        # Create fivegram, fourgram, trigram, bigram, and unigram\n",
    "        vocab = set()\n",
    "        all_words = list()\n",
    "        unigram = {}\n",
    "        bigram = {}\n",
    "        trigram = {}\n",
    "        fourgram = {}\n",
    "        fivegram = {}\n",
    "        \n",
    "        for line in tokenized_steps:\n",
    "            tokens = line \n",
    "            bigrams = list(ngrams(tokens, n=2))\n",
    "            trigrams = list(ngrams(tokens, n=3))\n",
    "            fourgrams = list(ngrams(tokens, n=4))\n",
    "            fivegrams = list(ngrams(tokens, n=5))\n",
    "\n",
    "            # Discover new word\n",
    "            for token in tokens:\n",
    "                if token not in vocab:\n",
    "                    vocab.add(token)\n",
    "                all_words.append(token)\n",
    "                token = tuple([token])\n",
    "                if token not in unigram:\n",
    "                    unigram[token ] = 1\n",
    "                else:\n",
    "                    unigram[token] += 1\n",
    "\n",
    "            for each_bigram in bigrams:\n",
    "                if each_bigram not in bigram:\n",
    "                    bigram[each_bigram] = 1\n",
    "                else:\n",
    "                    bigram[each_bigram] += 1\n",
    "                    \n",
    "            for each_trigram in trigrams:\n",
    "                if each_trigram not in trigram:\n",
    "                    trigram[each_trigram] = 1\n",
    "                else:\n",
    "                    trigram[each_trigram] += 1\n",
    "        \n",
    "            for each_fourgram in fourgrams:\n",
    "                if each_fourgram not in fourgram:\n",
    "                    fourgram[each_fourgram] = 1\n",
    "                else:\n",
    "                    fourgram[each_fourgram] += 1  \n",
    "            for each_fivegram in fivegrams:\n",
    "                if each_fivegram not in fivegram:\n",
    "                    fivegram[each_fivegram] = 1\n",
    "                else:\n",
    "                    fivegram[each_fivegram] += 1\n",
    "\n",
    "        return [all_words, vocab, unigram, bigram, trigram, fourgram, fivegram]\n",
    "    else:\n",
    "        print(\"Error! Provide a valid value for n.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to estimate bidirectional n-grams' probabilities\n",
    "def get_prob_unigram(word, count_all_tokens):\n",
    "    try:\n",
    "        return (unigram[tuple([word])]) / (count_all_tokens)\n",
    "    except:\n",
    "        # If word does not exist in vocab, return estimate for the 'unk' token\n",
    "        return (unigram[('<unk>',)]) / (count_all_tokens)\n",
    "\n",
    "\n",
    "def get_prob_bigram_before(words):\n",
    "    try:\n",
    "        return (bigram[words]) / (unigram[tuple([words[0]])])\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def get_prob_bigram_after(words):\n",
    "    try:\n",
    "        return (bigram[words]) / (unigram[tuple([words[-1]])])\n",
    "    except:\n",
    "        return 0    \n",
    "\n",
    "\n",
    "def get_prob_trigram_before(words):\n",
    "    try:\n",
    "        return (trigram[words]) / (bigram[words[:2]])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_prob_trigram_after(words):\n",
    "    try:\n",
    "        return (trigram[words]) / (bigram[words[-2:]])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_prob_fourgram_before(words):\n",
    "    try:\n",
    "        return (fourgram[words]) / (trigram[words[:3]])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_prob_fourgram_after(words):\n",
    "    try:\n",
    "        return (fourgram[words]) / (trigram[words[-3:]])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_prob_fivegram_before(words):\n",
    "    try:\n",
    "        return (fivegram[words]) / (fourgram[words[:4]])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_prob_fivegram_after(words):\n",
    "    try:\n",
    "        return (fivegram[words]) / (fourgram[words[-4:]])\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get the probability of a target word given the context words to the left (before) of the target word\n",
    "def get_probability_word_before(words_before, target_word, len_words_before, count_all_tokens, unique_vocab_len):\n",
    "    \"\"\"\" Get probability for 'target_word' using only context words that come before the word.\n",
    "    Use back-off if n-gram does not exist.\n",
    "    \n",
    "    \"\"\"\n",
    "    used_unigram = False\n",
    "\n",
    "    # Uses 5-gram\n",
    "    if len_words_before == 4:\n",
    "        p5 = get_prob_fivegram_before((words_before[-4], words_before[-3], words_before[-2], words_before[-1], target_word))\n",
    "        # If p5 == 0, 5-gram never occurred, try 4-gram\n",
    "        if p5 == 0:\n",
    "            p5 = 0.4*get_prob_fourgram_before((words_before[-3], words_before[-2], words_before[-1], target_word))\n",
    "            # If p4 == 0, 4-gram never occurred, try 3-gram\n",
    "            if p5 == 0:\n",
    "                p5 = 0.4*get_prob_trigram_before((words_before[-2], words_before[-1], target_word))\n",
    "                # If p3 == 0, 3-gram never occurred, try bigram\n",
    "                if p5 == 0:\n",
    "                    p5 = 0.4*get_prob_bigram_before((words_before[-1], target_word))\n",
    "                    # If p2 == 0, bigram never occurred, try unigram\n",
    "                    if p5 == 0:\n",
    "                        # Set variable to indicated that a unigram was used\n",
    "                        used_unigram = True\n",
    "                        p5 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return [p5,used_unigram]\n",
    "    \n",
    "    # Uses 4-gram\n",
    "    elif len_words_before == 3:\n",
    "        p4 = get_prob_fourgram_before((words_before[-3], words_before[-2], words_before[-1], target_word))\n",
    "        # If p4 == 0, 4-gram never occurred, try 3-gram\n",
    "        if p4 == 0:\n",
    "            p4 = 0.4*get_prob_trigram_before((words_before[-2], words_before[-1], target_word))\n",
    "            # If p4 == 0, 3-gram never occurred, try bigram\n",
    "            if p4 == 0:\n",
    "                p4 = 0.4*get_prob_bigram_before((words_before[-1], target_word))\n",
    "                # If p4 == 0, bigram never occurred, try unigram\n",
    "                if p4 == 0:\n",
    "                    # Set variable to indicated that a unigram was used\n",
    "                    used_unigram = True\n",
    "                    p4 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return [p4,used_unigram]\n",
    "    \n",
    "    # Uses trigram\n",
    "    elif len_words_before == 2:\n",
    "        p3 = get_prob_trigram_before((words_before[-2], words_before[-1], target_word))\n",
    "        # If p3 == 0, 3-gram never occurred, try bigram\n",
    "        if p3 == 0:\n",
    "            p3 = 0.4*get_prob_bigram_before((words_before[-1], target_word))\n",
    "            # If p3 == 0, bigram never occurred, try unigram\n",
    "            if p3 == 0:\n",
    "                # Set variable to indicated that a unigram was used\n",
    "                used_unigram = True\n",
    "                p3 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return [p3,used_unigram]\n",
    "    \n",
    "    # Uses bigram\n",
    "    elif len_words_before == 1:\n",
    "        p2 = get_prob_bigram_before((words_before[-1], target_word))\n",
    "        # If p2 == 0, bigram never occurred, try unigram\n",
    "        if p2 == 0:\n",
    "            # Set variable to indicated that a unigram was used\n",
    "            used_unigram = True\n",
    "            p2 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return [p2,used_unigram]\n",
    "    \n",
    "    # Uses unigram\n",
    "    else:\n",
    "        used_unigram = True\n",
    "        p1 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return [p1,used_unigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get the probability of a target word given the context words to the right (after) of the target word\n",
    "def get_probability_word_after(words_after, target_word, len_words_after, count_all_tokens, unique_vocab_len):\n",
    "    \"\"\"\" Get probability for 'target_word' using only context words that come after the word.\n",
    "    Use back-off if n-gram does not exist.\n",
    "    \n",
    "    \"\"\"  \n",
    "    used_unigram = False\n",
    "    \n",
    "    # Uses 5-gram\n",
    "    if len_words_after == 4:\n",
    "        p5 = get_prob_fivegram_after((target_word, words_after[0], words_after[1], words_after[2], words_after[3]))\n",
    "        # If p5 == 0, 5-gram never occurred, try 4-gram\n",
    "        if p5 == 0:\n",
    "            p5 = 0.4*get_prob_fourgram_after((target_word, words_after[0], words_after[1], words_after[2]))\n",
    "            # If p4 == 0, 4-gram never occurred, try 3-gram\n",
    "            if p5 == 0:\n",
    "                p5 = 0.4*get_prob_trigram_after((target_word, words_after[0], words_after[1]))\n",
    "                # If p3 == 0, 3-gram never occurred, try bigram\n",
    "                if p5 == 0:\n",
    "                    p5 = 0.4*get_prob_bigram_after((target_word, words_after[0]))\n",
    "                    # If p2 == 0, bigram never occurred, try unigram\n",
    "                    if p5 == 0:\n",
    "                        # Set variable to indicated that a unigram was used\n",
    "                        used_unigram = True\n",
    "                        p5 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return [p5,used_unigram]\n",
    "    \n",
    "    # Uses 4-gram\n",
    "    elif len_words_after == 3:\n",
    "        p4 = get_prob_fourgram_after((target_word, words_after[0], words_after[1], words_after[2]))\n",
    "        # If p4 == 0, 4-gram never occurred, try 3-gram\n",
    "        if p4 == 0:\n",
    "            p4 = 0.4*get_prob_trigram_after((target_word, words_after[0], words_after[1]))\n",
    "            # If p4 == 0, 3-gram never occurred, try bigram\n",
    "            if p4 == 0:\n",
    "                p4 = 0.4*get_prob_bigram_after((target_word, words_after[0]))\n",
    "                # If p4 == 0, bigram never occurred, try unigram\n",
    "                if p4 == 0: \n",
    "                    # Set variable to indicated that a unigram was used\n",
    "                    used_unigram = True\n",
    "                    p4 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return [p4,used_unigram]\n",
    "    \n",
    "    # Uses trigram\n",
    "    elif len_words_after == 2:\n",
    "        p3 = get_prob_trigram_after((target_word, words_after[0], words_after[1]))\n",
    "        # If p3 == 0, 3-gram never occurred, try bigram\n",
    "        if p3 == 0:\n",
    "            p3 = 0.4*get_prob_bigram_after((target_word, words_after[0]))\n",
    "            # If p3 == 0, bigram never occurred, try unigram\n",
    "            if p3 == 0:\n",
    "                # Set variable to indicated that a unigram was used\n",
    "                used_unigram = True\n",
    "                p3 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return [p3,used_unigram]\n",
    "    \n",
    "    # Uses bigram\n",
    "    elif len_words_after == 1:\n",
    "        p2 = get_prob_bigram_after((target_word, words_after[0]))\n",
    "        # If p2 == 0, bigram never occurred, try unigram\n",
    "        if p2 == 0:\n",
    "            # Set variable to indicated that a unigram was used\n",
    "            used_unigram = True\n",
    "            p2 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return [p2,used_unigram]\n",
    "    \n",
    "    # Uses unigram\n",
    "    else:\n",
    "        used_unigram = True\n",
    "        p1 = get_prob_unigram((target_word), count_all_tokens)\n",
    "        return [p1,used_unigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get the probability of a target word given the context words to the left (before) and right (after) of the target word - bidirectional\n",
    "def get_probability_word_bidir(words_before, words_after, target_word, count_all_tokens, unique_vocab):\n",
    "    \"\"\" Get probability of a 'target_word' given the context words 'words_before' and 'words_after'.\n",
    "    Use back-off: when a n-gram is not found (returned zero probability),\n",
    "    use (n-1)-gram, and so on.\n",
    "    \n",
    "    \"\"\"\n",
    "    unique_vocab_len = len(unique_vocab)    \n",
    "    len_words_before = len(words_before)\n",
    "    len_words_after = len(words_after)\n",
    "    \n",
    "    prob_before,used_unigram_bef = get_probability_word_before(words_before, target_word, len_words_before, count_all_tokens, unique_vocab_len)\n",
    "    prob_after,used_unigram_aft = get_probability_word_after(words_after, target_word, len_words_after, count_all_tokens, unique_vocab_len)\n",
    "    prob_final = (prob_before + prob_after)/2.0\n",
    "    \n",
    "    # Return True if either of prob_before or prob_after used unigram to make the prediction\n",
    "    used_unigram = used_unigram_bef or used_unigram_aft\n",
    "    return [prob_final,used_unigram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to go through all the words in the vocabulary and get the top-k most likely words\n",
    "def get_likely_word_backoff_bidirec(words_before, words_after, count_all_tokens, unique_vocab, num_words_recommend):\n",
    "    list_prob = []\n",
    "    for word in unique_vocab:\n",
    "        if (word == '[START]') or (word == '[END]'):\n",
    "            continue\n",
    "        prob,used_unigram = get_probability_word_bidir(words_before, words_after, word, count_all_tokens, unique_vocab)\n",
    "        list_prob.append((word, prob, used_unigram))\n",
    "        \n",
    "    # Sort by probability\n",
    "    list_prob.sort(key=lambda x: x[1], reverse=True)\n",
    "    return list_prob[:num_words_recommend]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build n-gram models with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_models = ngram_count(training_data_with_unk, 5)\n",
    "all_words = ngram_models[0]\n",
    "unique_vocab = ngram_models[1]\n",
    "unigram = ngram_models[2]\n",
    "bigram = ngram_models[3]\n",
    "trigram = ngram_models[4]\n",
    "fourgram = ngram_models[5]\n",
    "fivegram = ngram_models[6]\n",
    "\n",
    "count_all_tokens = len(all_words)\n",
    "unique_vocab_len = len(unique_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of words in the training corpus: \", count_all_tokens)\n",
    "print(\"Total number of unique words in the training corpus: \", unique_vocab_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT: Fine-tuned BERT large uncased whole word masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bert_model = BertForMaskedLM.from_pretrained('/fine_tuned_bert_models/my_bert_whole_word_stopwords')\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased-whole-word-masking')\n",
    "\n",
    "# Use the Huggingface pipeline 'fill-mask' with our fine-tuned model to make predictions efficiently\n",
    "# https://huggingface.co/transformers/main_classes/pipelines.html\n",
    "unmasker = pipeline('fill-mask', model=my_bert_model, tokenizer=bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute accuracy@k for the n-gram and BERT models separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram evaluation\n",
    "accuracy_k_ngram_dict = {}\n",
    "\n",
    "num_words_recommend = 10\n",
    "top3_accuracy_list = []\n",
    "top5_accuracy_list = []\n",
    "top10_accuracy_list = []\n",
    "\n",
    "for sentence in testing_data:\n",
    "    N = len(sentence)    \n",
    "    top3_suggestions = []\n",
    "    top5_suggestions = []\n",
    "    top10_suggestions = []\n",
    "    \n",
    "    for index in range(1,N-1):\n",
    "        token = sentence[index]\n",
    "            \n",
    "        if (index-4) >= 0:\n",
    "            composed_token_before = sentence[index-4 : index]\n",
    "        else:\n",
    "            composed_token_before = sentence[:index]\n",
    "\n",
    "        composed_token_after = sentence[index+1 : index+1 + 4]\n",
    "\n",
    "        likely_words_score = get_likely_word_backoff_bidirec(composed_token_before, composed_token_after, count_all_tokens, unique_vocab, num_words_recommend)\n",
    "        \n",
    "        # Compute accuracy@k\n",
    "        top3_likely_words = [x[0] for x in likely_words_score[:3]]\n",
    "        if token in top3_likely_words:\n",
    "            top3_suggestions.append(1)\n",
    "        else:\n",
    "            top3_suggestions.append(0)\n",
    "\n",
    "        top5_likely_words = [x[0] for x in likely_words_score[:5]]\n",
    "        if token in top5_likely_words:\n",
    "            top5_suggestions.append(1)\n",
    "        else:\n",
    "            top5_suggestions.append(0)\n",
    "            \n",
    "        top10_likely_words = [x[0] for x in likely_words_score[:10]]\n",
    "        if token in top10_likely_words:\n",
    "            top10_suggestions.append(1)\n",
    "        else:\n",
    "            top10_suggestions.append(0)\n",
    "\n",
    "    # Accuracy\n",
    "    top3_accuracy = sum(top3_suggestions)/len(top3_suggestions)\n",
    "    top3_accuracy_list.append(top3_accuracy)\n",
    "    \n",
    "    top5_accuracy = sum(top5_suggestions)/len(top5_suggestions)\n",
    "    top5_accuracy_list.append(top5_accuracy)\n",
    "    \n",
    "    top10_accuracy = sum(top10_suggestions)/len(top10_suggestions)\n",
    "    top10_accuracy_list.append(top10_accuracy)\n",
    "    \n",
    "accuracy_k_ngram_dict[3] = top3_accuracy_list\n",
    "accuracy_k_ngram_dict[5] = top5_accuracy_list\n",
    "accuracy_k_ngram_dict[10] = top10_accuracy_list\n",
    "\n",
    "\n",
    "# BERT evaluation\n",
    "accuracy_k_bert_dict = {}\n",
    "\n",
    "# Get more predictions than necessary as we will exclude some predictions\n",
    "num_words_recommend = 50\n",
    "top3_accuracy_list = []\n",
    "top5_accuracy_list = []\n",
    "top10_accuracy_list = []\n",
    "\n",
    "for sentence in testing_data_stopwords:\n",
    "    N = len(sentence)    \n",
    "    top3_suggestions = []\n",
    "    top5_suggestions = []\n",
    "    top10_suggestions = []\n",
    "    \n",
    "    for index in range(1,N-1):\n",
    "        token = sentence[index]\n",
    "\n",
    "        masked_sentence = []\n",
    "        for i in range(1,N-1):\n",
    "            if i == index:\n",
    "                masked_sentence.append('[MASK]')\n",
    "            else:\n",
    "                masked_sentence.append(sentence[i])\n",
    "        masked_sentence = ' '.join(masked_sentence)\n",
    "        masked_sentence += '.'\n",
    "\n",
    "        bert_predictions = unmasker(masked_sentence, top_k=(num_words_recommend + 150))\n",
    "        bert_predictions_words = []\n",
    "        for elem in bert_predictions:\n",
    "            predicted_token = elem['token_str']\n",
    "            # Skip recommendations of stop words, punctuations, and sub-words (indicated by ##)\n",
    "            if (predicted_token not in string.punctuation) and (predicted_token != '—') and ('##' not in predicted_token):\n",
    "                bert_predictions_words.append(predicted_token)\n",
    "            else:\n",
    "                continue\n",
    "            if len(bert_predictions_words) == num_words_recommend:\n",
    "                break\n",
    "    \n",
    "        # Compute accuracy@k\n",
    "        top3_likely_words = bert_predictions_words[:3]\n",
    "        if token in top3_likely_words:\n",
    "            top3_suggestions.append(1)\n",
    "        else:\n",
    "            top3_suggestions.append(0)\n",
    "\n",
    "        top5_likely_words = bert_predictions_words[:5]\n",
    "        if token in top5_likely_words:\n",
    "            top5_suggestions.append(1)\n",
    "        else:\n",
    "            top5_suggestions.append(0)\n",
    "\n",
    "        top10_likely_words = bert_predictions_words[:10]\n",
    "        if token in top10_likely_words:\n",
    "            top10_suggestions.append(1)\n",
    "        else:\n",
    "            top10_suggestions.append(0)\n",
    "\n",
    "    # Accuracy\n",
    "    top3_accuracy = sum(top3_suggestions)/len(top3_suggestions)\n",
    "    top3_accuracy_list.append(top3_accuracy)\n",
    "    \n",
    "    top5_accuracy = sum(top5_suggestions)/len(top5_suggestions)\n",
    "    top5_accuracy_list.append(top5_accuracy)\n",
    "    \n",
    "    top10_accuracy = sum(top10_suggestions)/len(top10_suggestions)\n",
    "    top10_accuracy_list.append(top10_accuracy)\n",
    "    \n",
    "accuracy_k_bert_dict[3] = top3_accuracy_list\n",
    "accuracy_k_bert_dict[5] = top5_accuracy_list\n",
    "accuracy_k_bert_dict[10] = top10_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print n-gram results\n",
    "print(st.mean(accuracy_k_ngram_dict[3]))\n",
    "print(st.mean(accuracy_k_ngram_dict[5]))\n",
    "print(st.mean(accuracy_k_ngram_dict[10]))\n",
    "\n",
    "print(st.median(accuracy_k_ngram_dict[3]))\n",
    "print(st.median(accuracy_k_ngram_dict[5]))\n",
    "print(st.median(accuracy_k_ngram_dict[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print BERT results\n",
    "print(st.mean(accuracy_k_bert_dict[3]))\n",
    "print(st.mean(accuracy_k_bert_dict[5]))\n",
    "print(st.mean(accuracy_k_bert_dict[10]))\n",
    "\n",
    "print(st.median(accuracy_k_bert_dict[3]))\n",
    "print(st.median(accuracy_k_bert_dict[5]))\n",
    "print(st.median(accuracy_k_bert_dict[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframse with accuracy\n",
    "df = pd.DataFrame(accuracy_k_ngram_dict[3], columns =['Accuracy'])\n",
    "df.to_csv('ngram_bert_recommendation/ngram_accuracy_top3.csv', index = False)\n",
    "\n",
    "df = pd.DataFrame(accuracy_k_ngram_dict[5], columns =['Accuracy'])\n",
    "df.to_csv('ngram_bert_recommendation/ngram_accuracy_top5.csv', index = False)\n",
    "\n",
    "df = pd.DataFrame(accuracy_k_ngram_dict[10], columns =['Accuracy'])\n",
    "df.to_csv('ngram_bert_recommendation/ngram_accuracy_top10.csv', index = False)\n",
    "\n",
    "df = pd.DataFrame(accuracy_k_bert_dict[3], columns =['Accuracy'])\n",
    "df.to_csv('ngram_bert_recommendation/bert_accuracy_top3.csv', index = False)\n",
    "\n",
    "df = pd.DataFrame(accuracy_k_bert_dict[5], columns =['Accuracy'])\n",
    "df.to_csv('ngram_bert_recommendation/bert_accuracy_top5.csv', index = False)\n",
    "\n",
    "df = pd.DataFrame(accuracy_k_bert_dict[10], columns =['Accuracy'])\n",
    "df.to_csv('ngram_bert_recommendation/bert_accuracy_top10.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram + BERT\n",
    "Combining n-gram and BERT based on the observations drawn from a manual analysis on a sample in which one model fails (accuracy=0) and the other succeeds (accuracy=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For efficiency, store BERT predictions for each test sentence in a read-only dict\n",
    "bert_predictions_dict = {}\n",
    "\n",
    "# Suggest top-3 most likely words\n",
    "num_words_recommend = 100\n",
    "top3_accuracy_list = []\n",
    "top5_accuracy_list = []\n",
    "top10_accuracy_list = []\n",
    "\n",
    "for main_index in range(len(testing_data_stopwords)):\n",
    "    sentence_bert = testing_data_stopwords[main_index]\n",
    "    \n",
    "    N = len(sentence_bert)    \n",
    "    top3_suggestions = []\n",
    "    top5_suggestions = []\n",
    "    top10_suggestions = []\n",
    "\n",
    "    prediction_index_sentence = 0\n",
    "    for index in range(1,N-1):\n",
    "        token = sentence_bert[index]\n",
    "\n",
    "        masked_sentence = []\n",
    "        for i in range(1,N-1):\n",
    "            if i == index:\n",
    "                masked_sentence.append('[MASK]')\n",
    "            else:\n",
    "                masked_sentence.append(sentence_bert[i])\n",
    "        masked_sentence = ' '.join(masked_sentence)\n",
    "        masked_sentence += '.'\n",
    "\n",
    "        bert_predictions = unmasker(masked_sentence, top_k=num_words_recommend)\n",
    "        bert_predictions_words = []\n",
    "        for elem in bert_predictions:\n",
    "            predicted_token = elem['token_str']\n",
    "            if (predicted_token not in string.punctuation) and (predicted_token != '—') and ('##' not in predicted_token):\n",
    "                bert_predictions_words.append(predicted_token)\n",
    "            else:\n",
    "                continue\n",
    "            if len(bert_predictions_words) == 10:\n",
    "                break\n",
    "            \n",
    "        if tuple(sentence_bert) in bert_predictions_dict:\n",
    "            current_dict = bert_predictions_dict[tuple(sentence_bert)]\n",
    "            current_dict[prediction_index_sentence] = bert_predictions_words\n",
    "            bert_predictions_dict[tuple(sentence_bert)] = current_dict\n",
    "        else:\n",
    "            bert_predictions_dict[tuple(sentence_bert)] = {prediction_index_sentence : bert_predictions_words}\n",
    "        \n",
    "        prediction_index_sentence += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram + BERT prediction\n",
    "accuracy_k_ngram_bert_dict = {}\n",
    "\n",
    "# Suggest top-3 most likely words\n",
    "num_words_recommend = 10\n",
    "top3_accuracy_list = []\n",
    "top5_accuracy_list = []\n",
    "top10_accuracy_list = []\n",
    "\n",
    "\n",
    "for sentence in testing_data_stopwords:\n",
    "    N = len(sentence)    \n",
    "    top3_suggestions = []\n",
    "    top5_suggestions = []\n",
    "    top10_suggestions = []\n",
    "\n",
    "    prediction_index_sentence = 0\n",
    "    for index in range(1,N-1):\n",
    "        token = sentence[index]\n",
    "            \n",
    "        # Get predictions from n-gram\n",
    "        if (index-4) >= 0:\n",
    "            composed_token_before = sentence[index-4 : index]\n",
    "        else:\n",
    "            composed_token_before = sentence[:index]\n",
    "\n",
    "        composed_token_after = sentence[index+1 : index+1 + 4]\n",
    "\n",
    "        likely_words_score = get_likely_word_backoff_bidirec(composed_token_before, composed_token_after, count_all_tokens, unique_vocab, num_words_recommend)\n",
    "        \n",
    "        # Check if the n-gram backed off to the unigram and if the unigram prediction probability is below 0.5\n",
    "        if any(likely_words_score[:][2]) and (likely_words_score[0][1] < 0.5):\n",
    "            \n",
    "            # Get BERT predictions as n-gram prediction confidence score is below the threshold and the n-gram backed off to the unigram\n",
    "            try:\n",
    "                bert_predictions_words = bert_predictions_dict[tuple(sentence)][prediction_index_sentence]\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            top3_likely_words = bert_predictions_words[:3]\n",
    "            if token in top3_likely_words:\n",
    "                top3_suggestions.append(1)\n",
    "            else:\n",
    "                top3_suggestions.append(0)\n",
    "\n",
    "            top5_likely_words = bert_predictions_words[:5]\n",
    "            if token in top5_likely_words:\n",
    "                top5_suggestions.append(1)\n",
    "            else:\n",
    "                top5_suggestions.append(0)\n",
    "\n",
    "            top10_likely_words = bert_predictions_words[:10]\n",
    "            if token in top10_likely_words:\n",
    "                top10_suggestions.append(1)\n",
    "            else:\n",
    "                top10_suggestions.append(0)            \n",
    "\n",
    "        else:\n",
    "\n",
    "            # Keep the n-gram\n",
    "            top3_likely_words = [x[0] for x in likely_words_score[:3]]\n",
    "            if token in top3_likely_words:\n",
    "                top3_suggestions.append(1)\n",
    "            else:\n",
    "                top3_suggestions.append(0)\n",
    "\n",
    "            top5_likely_words = [x[0] for x in likely_words_score[:5]]\n",
    "            if token in top5_likely_words:\n",
    "                top5_suggestions.append(1)\n",
    "            else:\n",
    "                top5_suggestions.append(0)\n",
    "\n",
    "            top10_likely_words = [x[0] for x in likely_words_score[:10]]\n",
    "            if token in top10_likely_words:\n",
    "                top10_suggestions.append(1)\n",
    "            else:\n",
    "                top10_suggestions.append(0)   \n",
    "\n",
    "        prediction_index_sentence += 1\n",
    "\n",
    "    # Accuracy\n",
    "    top3_accuracy = sum(top3_suggestions)/len(top3_suggestions)\n",
    "    top3_accuracy_list.append(top3_accuracy)\n",
    "\n",
    "    top5_accuracy = sum(top5_suggestions)/len(top5_suggestions)\n",
    "    top5_accuracy_list.append(top5_accuracy)\n",
    "\n",
    "    top10_accuracy = sum(top10_suggestions)/len(top10_suggestions)\n",
    "    top10_accuracy_list.append(top10_accuracy)\n",
    "\n",
    "\n",
    "accuracy_k_ngram_bert_dict[3] = top3_accuracy_list\n",
    "accuracy_k_ngram_bert_dict[5] = top5_accuracy_list\n",
    "accuracy_k_ngram_bert_dict[10] = top10_accuracy_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print (n-gram + BERT) results\n",
    "print(st.mean(accuracy_k_ngram_bert_dict[3]))\n",
    "print(st.mean(accuracy_k_ngram_bert_dict[5]))\n",
    "print(st.mean(accuracy_k_ngram_bert_dict[10]))\n",
    "\n",
    "print(st.median(accuracy_k_ngram_bert_dict[3]))\n",
    "print(st.median(accuracy_k_ngram_bert_dict[5]))\n",
    "print(st.median(accuracy_k_ngram_bert_dict[10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
