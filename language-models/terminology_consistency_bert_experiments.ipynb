{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology improvement analysis through language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we perform experiments with different language models to improve the terminology of manual test case descriptions. We use the following type of language models:\n",
    "\n",
    "* Neural language models:\n",
    "  * We evaluate the following pre-trained models and their fine-tuned versions:\n",
    "    * BERT-based-uncased\n",
    "    * DistilBERT-based-uncased\n",
    "    * BERT large uncased whole word masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Language Model (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import statistics as st\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "from statistics import mean\n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "import collections\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "from expects import (contain_exactly, equal, expect, have_keys)\n",
    "import attr\n",
    "from functools import partial\n",
    "from tabulate import tabulate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.offline as offline\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-processed data\n",
    "For the terminology improvement module, we do not perform stop word removal and lemmatization for the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'training_testing_data/with_name_objective/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "read_handle = open(data_dir + 'training_data_stopwords.txt', 'r')\n",
    "for line in read_handle:\n",
    "    line = line.replace('\\n', '').split(',')\n",
    "    training_data.append(line)\n",
    "print(len(training_data))\n",
    "\n",
    "testing_data = []\n",
    "read_handle = open(data_dir + 'testing_data_stopwords.txt', 'r')\n",
    "for line in read_handle:\n",
    "    line = line.replace('\\n', '').split(',')\n",
    "    testing_data.append(line)\n",
    "print(len(testing_data))\n",
    "read_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute perplexity using cross-entropy loss of the BERT-based masked language model\n",
    "def calculate_perplexity_neural_model_direct(sentences: list, bert_model, bert_model_name, bert_tokenizer):\n",
    "    perplexity_neural_df = pd.DataFrame(columns = ['perplexity_score', 'model'])\n",
    "    index_to_add = 0\n",
    "    for sentence in tqdm(sentences):        \n",
    "        cross_entropy_loss_list = []\n",
    "        clean_sentence = sentence.copy()[1:-1]\n",
    "        \n",
    "        labels = clean_sentence.copy()\n",
    "        labels = ' '.join(labels)\n",
    "        labels = bert_tokenizer(labels, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        clean_sentence = ' '.join(clean_sentence)\n",
    "            \n",
    "        # Tokenizer\n",
    "        tokenized_sentence = bert_tokenizer(clean_sentence, return_tensors=\"pt\")\n",
    "        \n",
    "        # Get output (logist) and loss from model\n",
    "        for index in range(1, len(tokenized_sentence[\"input_ids\"][0])-1):\n",
    "            masked_sentence = bert_tokenizer(clean_sentence, return_tensors=\"pt\")\n",
    "            masked_sentence[\"input_ids\"][0][index] = 103\n",
    "            \n",
    "            outputs = bert_model(**masked_sentence, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            cross_entropy_loss_list.append(loss.item())\n",
    "            \n",
    "        # Get average cross-entropy loss per word and compute perplexity\n",
    "        cross_entropy_loss = mean(cross_entropy_loss_list)\n",
    "        perplexity_neural = math.exp(cross_entropy_loss)\n",
    "        \n",
    "        perplexity_neural_df.loc[index_to_add] = [perplexity_neural, bert_model_name]\n",
    "        index_to_add += 1\n",
    "\n",
    "    return perplexity_neural_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT base uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_neural_df_direct = calculate_perplexity_neural_model_direct(testing_data, model, \"BERT base uncased\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers if necessary\n",
    "# Q1 = perplexity_neural_df_direct['perplexity_score'].quantile(0.25)\n",
    "# Q3 = perplexity_neural_df_direct['perplexity_score'].quantile(0.75)\n",
    "# IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "# filter = (perplexity_neural_df_direct['perplexity_score'] >= Q1 - 1.5 * IQR) & (perplexity_neural_df_direct['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "# perplexity_neural_df_direct_outlier = perplexity_neural_df_direct.loc[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for quick visualization\n",
    "pio.renderers.default = 'iframe' # or 'notebook' or 'colab' or 'jupyterlab'\n",
    "fig = go.Figure()\n",
    "fig = px.box(perplexity_neural_df_direct_outlier, x=\"model\", y=\"perplexity_score\", points=False)\n",
    "fig.update_layout(\n",
    "    title=\"BERT base uncased\",\n",
    "    title_x=0.5,\n",
    "    xaxis_title=\"Model\",\n",
    "    yaxis_title=\"Perplexity metric\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"RebeccaPurple\"\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes as CSV to be read by R and generate plots for the paper\n",
    "perplexity_neural_df_direct.to_csv('pretrained_bert/perplexity_bert_base.csv', index=False)\n",
    "perplexity_neural_df_direct_outlier.to_csv('pretrained_bert/perplexity_bert_base_outlier.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DistilBERT base uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_neural_df_distill_bert_direct = calculate_perplexity_neural_model_direct(testing_data, model, \"DistilBERT base uncased\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers if necessary\n",
    "# Q1 = perplexity_neural_df_distill_bert_direct['perplexity_score'].quantile(0.25)\n",
    "# Q3 = perplexity_neural_df_distill_bert_direct['perplexity_score'].quantile(0.75)\n",
    "# IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "# filter = (perplexity_neural_df_distill_bert_direct['perplexity_score'] >= Q1 - 1.5 * IQR) & (perplexity_neural_df_distill_bert_direct['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "# perplexity_neural_df_distill_bert_direct_outlier = perplexity_neural_df_distill_bert_direct.loc[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for quick visualization\n",
    "pio.renderers.default = 'iframe' # or 'notebook' or 'colab' or 'jupyterlab'\n",
    "fig = go.Figure()\n",
    "fig = px.box(perplexity_neural_df_distill_bert_direct_outlier, x=\"model\", y=\"perplexity_score\", points=False)\n",
    "fig.update_layout(\n",
    "    title=\"DistilBERT base uncased\",\n",
    "    title_x=0.5,\n",
    "    xaxis_title=\"Model\",\n",
    "    yaxis_title=\"Perplexity metric\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"RebeccaPurple\"\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes as CSV to be read by R and generate plots for the paper\n",
    "perplexity_neural_df_distill_bert_direct.to_csv('pretrained_bert/perplexity_distilbert_base.csv', index=False)\n",
    "perplexity_neural_df_distill_bert_direct_outlier.to_csv('pretrained_bert/perplexity_distilbert_base_outlier.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT large uncased whole word masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-large-uncased-whole-word-masking')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_neural_df_whole_word_direct = calculate_perplexity_neural_model_direct(testing_data, model, \"BERT large uncased (whole word masking)\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers if necessary\n",
    "# Q1 = perplexity_neural_df_whole_word_direct['perplexity_score'].quantile(0.25)\n",
    "# Q3 = perplexity_neural_df_whole_word_direct['perplexity_score'].quantile(0.75)\n",
    "# IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "# filter = (perplexity_neural_df_whole_word_direct['perplexity_score'] >= Q1 - 1.5 * IQR) & (perplexity_neural_df_whole_word_direct['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "# perplexity_neural_df_whole_word_direct_outlier = perplexity_neural_df_whole_word_direct.loc[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for quick visualization\n",
    "pio.renderers.default = 'iframe' # or 'notebook' or 'colab' or 'jupyterlab'\n",
    "fig = go.Figure()\n",
    "fig = px.box(perplexity_neural_df_whole_word_direct_outlier, x=\"model\", y=\"perplexity_score\", points=False)\n",
    "fig.update_layout(\n",
    "    title=\"BERT whole word masking\",\n",
    "    title_x=0.5,\n",
    "    xaxis_title=\"Model\",\n",
    "    yaxis_title=\"Perplexity metric\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"RebeccaPurple\"\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes as CSV to be read by R and generate plots for the paper\n",
    "perplexity_neural_df_whole_word_direct.to_csv('pretrained_bert/perplexity_bert_whole_word.csv', index=False)\n",
    "perplexity_neural_df_whole_word_direct_outlier.to_csv('pretrained_bert/perplexity_bert_whole_word_outlier.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuned models\n",
    "The code for fine-tuning the models and the obtained fine-tuned models are in the 'fine-tune' directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataframe to store perplexity scores\n",
    "perplexity_bert_df = pd.DataFrame(columns = ['perplexity_score', 'model', 'stopwords'])\n",
    "index_to_add = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute perplexity using cross-entropy loss of the BERT-based masked language model\n",
    "def calculate_perplexity_fine_tuned_bert(sentences, bert_model, bert_tokenizer, df, model_name, stopwords_status):\n",
    "    global index_to_add\n",
    "    \n",
    "    for sentence in tqdm(sentences):        \n",
    "        cross_entropy_loss_list = []\n",
    "        clean_sentence = sentence.copy()[1:-1]\n",
    "    \n",
    "        labels = clean_sentence.copy()\n",
    "        labels = ' '.join(labels)\n",
    "        labels = bert_tokenizer(labels, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        \n",
    "        clean_sentence = ' '.join(clean_sentence) \n",
    "        tokenized_sentence = bert_tokenizer(clean_sentence, return_tensors=\"pt\")\n",
    "        \n",
    "        for index in range(1, len(tokenized_sentence[\"input_ids\"][0])-1):\n",
    "            masked_sentence = bert_tokenizer(clean_sentence, return_tensors=\"pt\")\n",
    "            masked_sentence[\"input_ids\"][0][index] = 103\n",
    "            \n",
    "            outputs = bert_model(**masked_sentence, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            cross_entropy_loss_list.append(loss.item())\n",
    "            \n",
    "        cross_entropy_loss = mean(cross_entropy_loss_list)\n",
    "        perplexity_neural = math.exp(cross_entropy_loss)\n",
    "        \n",
    "        df.loc[index_to_add] = [perplexity_neural, model_name, stopwords_status]\n",
    "        index_to_add += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuned - BERT base uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model weights\n",
    "my_bert_model = BertForMaskedLM.from_pretrained('/fine_tuned_bert_models/my_bert_base_stopwords')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity\n",
    "calculate_perplexity_fine_tuned_bert(testing_data, my_bert_model, tokenizer, perplexity_bert_df, \"BERT base uncased\", \"With stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuned - DistilBERT base uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model weights\n",
    "my_bert_model = BertForMaskedLM.from_pretrained('/fine_tuned_bert_models/my_bert_distilbert_stopwords')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity\n",
    "calculate_perplexity_fine_tuned_bert(testing_data, my_bert_model, tokenizer, perplexity_bert_df, \"DistilBERT base uncased\", \"With stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuned - BERT large uncased whole word masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model weights\n",
    "my_bert_model = BertForMaskedLM.from_pretrained('/fine_tuned_bert_models/my_bert_whole_word_stopwords')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity\n",
    "calculate_perplexity_fine_tuned_bert(testing_data, my_bert_model, tokenizer, perplexity_bert_df, \"BERT large uncased (whole word masking)\", \"With stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers all at once if necessary\n",
    "# Q1 = perplexity_bert_df['perplexity_score'].quantile(0.25)\n",
    "# Q3 = perplexity_bert_df['perplexity_score'].quantile(0.75)\n",
    "# IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "# filter = (perplexity_bert_df['perplexity_score'] >= Q1 - 1.5 * IQR) & (perplexity_bert_df['perplexity_score'] <= Q3 + 1.5 *IQR)\n",
    "# perplexity_bert_df_outlier = perplexity_bert_df.loc[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for quick visualization\n",
    "pio.renderers.default = 'iframe' # or 'notebook' or 'colab' or 'jupyterlab'\n",
    "fig = go.Figure()\n",
    "fig = px.box(perplexity_bert_df_outlier, x=\"model\", y=\"perplexity_score\", color=\"stopwords\", points=False)\n",
    "fig.update_layout(\n",
    "    title=\"Fine-tuned BERT\",\n",
    "    title_x=0.5,\n",
    "    xaxis_title=\"Model\",\n",
    "    yaxis_title=\"Perplexity metric\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"RebeccaPurple\"\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes as CSV to be read by R and generate plots for the paper\n",
    "perplexity_bert_df.to_csv('fine_tuned_bert/perplexity_finetuned_bert.csv', index=False)\n",
    "perplexity_bert_df_outlier.to_csv('fine_tuned_bert/perplexity_finetuned_bert_outlier.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
